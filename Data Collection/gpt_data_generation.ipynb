{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own attempt at parallelizing OpenAI API requests\n",
    "\n",
    "Found this after I made an earlier version of this: https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py \n",
    "\n",
    "Requires data in jsonl format, which is inconveinient at this time. Might move to this in the future to take advantage of being able to explicitly set rate limits and abide by those rules. In this implementation, I'm eyeballing the rate/token limits. Trying to operate in 50-75% of each territory as to not get messed up results... ymmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from itertools import chain\n",
    "import concurrent.futures\n",
    "\n",
    "test = False # sample or run on full dataset?\n",
    "sample_size = 4 # if test=True\n",
    "\n",
    "scape_output = pickle.load(open('data/scraper_output.p', 'rb'))\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if test: \n",
    "    keys = random.sample(list(scape_output.keys()), sample_size)\n",
    "    scape_output = {key: scape_output[key] for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(key, value, question_count):\n",
    "    # record the time before the request is sent\n",
    "    start_time = time.time()\n",
    "\n",
    "    soup = BeautifulSoup(value.content, \"html.parser\")\n",
    "    cleaned = re.sub('[\\n]+', '\\n', soup.text.strip())\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the cleaned HTML given below, generate as many questions possible with their answers.\n",
    "    Try to make the questions relevant from the perspective of a prospective or current student, as well as faculty and staff.\n",
    "    Format your responce in JSON, with the \"instruction\" field containing the question, an empty \"input\" field, and the answer in the \"output\" field.\n",
    "    Include up to {question_count} questions, each being a sentence or two long in length. Do not include question number.\n",
    "    Keep answers somewhat brief, but be enthusiastic in your response!\\n\\n\"\"\"\n",
    "    \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful question answer generator.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\\n This is the cleaned HTML: \\n{cleaned}\\n Start:.\"},\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    # calculate the time it took to receive the response\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    qa = completion.choices[0].message.content\n",
    "    tokens = completion.usage[\"total_tokens\"]\n",
    "\n",
    "    print(f\"Success! Complete in {response_time:.2f}s with {tokens} tokens for {key}\")\n",
    "\n",
    "    return key, qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Complete in 31.36s with 2572 tokens for https://www2.brockport.edu/support/administration-finance/enterprise-risk-management\n",
      "Success! Complete in 33.47s with 2456 tokens for https://www2.brockport.edu/support/parking/campus-parking/faculty\n",
      "Success! Complete in 35.10s with 1677 tokens for https://www2.brockport.edu/about/contact_us\n",
      "Success! Complete in 35.40s with 2366 tokens for https://www2.brockport.edu/live/profiles/5407-student-injury-policy\n",
      "Success! Complete in 35.66s with 2555 tokens for https://www2.brockport.edu/academics/school-business-management/?program=marketing-major-minor\n",
      "Success! Complete in 36.46s with 3198 tokens for https://www2.brockport.edu/academics/neuroscience/directory\n",
      "Success! Complete in 39.28s with 4097 tokens for https://www2.brockport.edu/live/blurbs/2080-adirondack-cc-course-equivalencies\n",
      "Success! Complete in 41.34s with 2160 tokens for https://www2.brockport.edu/academics/computing-sciences/careers-computing\n",
      "Success! Complete in 41.94s with 2276 tokens for https://www2.brockport.edu/academics/sociology\n",
      "Success! Complete in 44.77s with 4097 tokens for https://www2.brockport.edu/live/profiles/1707-logan-rath\n",
      "Success! Complete in 45.35s with 2187 tokens for https://www2.brockport.edu/live/profiles/1817-d-malsegna\n",
      "Success! Complete in 52.82s with 2414 tokens for https://www2.brockport.edu/academics/mcnair/mission\n",
      "Success! Complete in 53.16s with 2702 tokens for https://www2.brockport.edu/academics/kinesiology/kinesiology/about\n",
      "Success! Complete in 57.19s with 2526 tokens for https://www2.brockport.edu/life/student-conduct/directory\n",
      "Success! Complete in 63.75s with 2716 tokens for https://www2.brockport.edu/academics/kinesiology/exercise-science/experiential-learning\n",
      "Success! Complete in 103.42s with 3373 tokens for https://www2.brockport.edu/scholarships-aid/scholarships/rotc\n"
     ]
    }
   ],
   "source": [
    "gpt_output = {}\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    future_to_item = {executor.submit(process_item, key, value, 25): key for key, value in scape_output.items()}\n",
    "    for future in concurrent.futures.as_completed(future_to_item):\n",
    "        key = future_to_item[future]\n",
    "        try:\n",
    "            key, qa = future.result()\n",
    "            gpt_output[key] = qa\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (key, exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gpt_output, open('data/gpt_output.p', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse GPT output to JSON\n",
    "\n",
    "While the prompt above to translate clean HTML to question/answer format does specify to do it in JSON format, GPT3.5 does not always do it perfectly. However, it always get close. Instead of trying to fix the JSON output from GPT, In this step I'm using regex to parse for all the instructions (questions), and outputs (answers). This is returned into a python list of dictionaries, which is appended for each webpage. Eventually I shuffle this so the questions are all mixed up instead of grouped by webpage, and dump it to a json file.\n",
    "\n",
    "For any questions which seem off, investigate the original webpage. I've left gpt_output as a python dictionary specifically for this reason, so that we can always refer back to the data and know exactly where it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regular expression pattern for a JSON object with \"instruction\" and \"output\"\n",
    "pattern = r'\"instruction\":\\s*\"(.*?)\",.*?\"output\":\\s*\"(.*?)\"'\n",
    "\n",
    "def extract_data(s):\n",
    "    matches = re.findall(pattern, s, flags=re.DOTALL)\n",
    "    data = [{\"instruction\": m[0], \"input\": \"\", \"output\": m[1]} for m in matches]\n",
    "    return data\n",
    "\n",
    "jsonqa = []\n",
    "\n",
    "for key, value in gpt_output.items():\n",
    "    clean_value = extract_data(value)\n",
    "    jsonqa.append(clean_value)\n",
    "\n",
    "jsonqa = list(chain(*jsonqa))\n",
    "\n",
    "random.shuffle(jsonqa)\n",
    "\n",
    "# Write to a JSON file\n",
    "with open('data/gpt_output_json.json', 'w') as f:\n",
    "    json.dump(jsonqa, f, indent=4)  # Dump the entire list at once"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
