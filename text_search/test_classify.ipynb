{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from question_classifier import QuestionClassifier\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class QuestionClassifierWrapper:\n",
    "    \"Gets a question and using the probability output returns the exact places to do text retrieval from.\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            main_categorization_model_dir: str = \"model\",\n",
    "            subcategorization_model_dir: str = \"subcat_models/\"\n",
    "        ):\n",
    "        self.categorized_data = load_dataset(\"msaad02/categorized-data\", split=\"train\").to_pandas()\n",
    "        embeddings = pickle.load(open(\"embeddings.pickle\", \"rb\"))\n",
    "        self.data = embeddings['data']\n",
    "        self.embeddings = embeddings['embeddings']\n",
    "        self.main_classifier = QuestionClassifier(model_dir=main_categorization_model_dir)\n",
    "        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "        self.reranker_model = SentenceTransformer('msmarco-distilbert-base-v4', device='cuda')\n",
    "        self.subcategory_classifiers = {}\n",
    "        for subcat in os.listdir(subcategorization_model_dir):\n",
    "            self.subcategory_classifiers[subcat] = QuestionClassifier(subcategorization_model_dir + subcat)\n",
    "\n",
    "    def _predict(self, question: str, return_probabilities: bool = False):\n",
    "        \"Raw interface between the classifier and the user.\"\n",
    "        prediction = {}\n",
    "        if return_probabilities:\n",
    "            prediction['category'], prediction['main_probs'] = self.main_classifier.predict(question, True)\n",
    "        else:\n",
    "            prediction['category'] = self.main_classifier.predict(question)\n",
    "\n",
    "        category = prediction['category']\n",
    "        if category in self.subcategory_classifiers:\n",
    "            subcategory_classifier = self.subcategory_classifiers[category]\n",
    "\n",
    "            if return_probabilities:\n",
    "                prediction['subcategory'], sub_probs = subcategory_classifier.predict(question, True)\n",
    "                prediction['sub_probs'] = {f'{category}-{subcat}': prob for subcat, prob in sub_probs.items()}\n",
    "            else:\n",
    "                prediction['subcategory'] = subcategory_classifier.predict(question)\n",
    "        return prediction\n",
    "    \n",
    "    def _get_text_retrieval_places(self, question: str):\n",
    "        \"\"\"\n",
    "        High level interface between the classifier and the user. Tells us where to do text retrieval from. Based on the probability output of the categorization models.\n",
    "\n",
    "        It does this by returning the top categories with confidence > 0.2 of the highest probability category. (I refer to confidence as the model's probability output.)\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'main_categories': [str],\n",
    "                'subcategories': [str]\n",
    "            }\n",
    "        \"\"\"\n",
    "        prediction = self._predict(question, True)\n",
    "\n",
    "        # main category\n",
    "        main_cat_probs_df = pd.DataFrame(\n",
    "            [(category, prob) for category, prob in prediction['main_probs'].items()], \n",
    "            columns=['category', 'probability']\n",
    "        ).sort_values(by='probability', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Use all categories at the top within 0.2 of the best category\n",
    "\n",
    "        # Highest category probability\n",
    "        max_main_prob = main_cat_probs_df['probability'][0]\n",
    "\n",
    "        # Categories within 0.2 of the highest category\n",
    "        main_categories_to_use = main_cat_probs_df[main_cat_probs_df['probability'] > max_main_prob - 0.2]['category'].tolist()\n",
    "\n",
    "        if 'sub_probs' in prediction.keys():\n",
    "            subcategory_probs_df = pd.DataFrame(\n",
    "                [(category, prob) for category, prob in prediction['sub_probs'].items()], \n",
    "                columns=['category', 'probability']\n",
    "            ).sort_values(by='probability', ascending=False).reset_index(drop=True)\n",
    "\n",
    "            # Highest subcategory probability\n",
    "            max_sub_prob = subcategory_probs_df['probability'][0]\n",
    "\n",
    "            # Subcategories within 0.2 of the highest subcategory\n",
    "            subcategories_to_use = subcategory_probs_df[subcategory_probs_df['probability'] > max_sub_prob - 0.2]['category'].tolist()\n",
    "\n",
    "        text_retreival_places = {\n",
    "            'main_categories': main_categories_to_use,\n",
    "            'subcategories': subcategories_to_use if 'sub_probs' in prediction.keys() else []\n",
    "        }\n",
    "\n",
    "        return text_retreival_places\n",
    "\n",
    "    def retreive_text(self, question: str, top_n: int = 10):\n",
    "        \"\"\"\n",
    "        This is the last step of retreival. The next (and final) step is to rerank the results using the reranker model.\n",
    "\n",
    "        The output of this model is the top n results using semantic search. The results it is pulling from are the ones that are in the categories returned by the _get_text_retrieval_places function, which itself is using the probability output of the categorization models.\n",
    "        \"\"\"\n",
    "\n",
    "        text_retrieval_places = self._get_text_retrieval_places(question)\n",
    "        \n",
    "        question_embedding = self.embedding_model.encode(\"tell me about the nursing program\", normalize_embeddings=True)\n",
    "\n",
    "        text_embedding_for_question = []\n",
    "        raw_text_for_question = []\n",
    "\n",
    "        for category in text_retrieval_places['main_categories']:\n",
    "            if category in self.embeddings.keys():\n",
    "                text_embedding_for_question.extend(self.embeddings[category])\n",
    "                raw_text_for_question.extend(self.data[category])\n",
    "            else:\n",
    "                print(f\"Category {category} not found in the embeddings dictionary.\")\n",
    "\n",
    "        for subcategory in text_retrieval_places['subcategories']:\n",
    "            if subcategory in self.embeddings.keys():\n",
    "                text_embedding_for_question.extend(self.embeddings[subcategory])\n",
    "                raw_text_for_question.extend(self.data[subcategory])\n",
    "            else:\n",
    "                print(f\"Subcategory {subcategory} not found in the embeddings dictionary.\")\n",
    "\n",
    "        similarity = text_embedding_for_question @ question_embedding.T\n",
    "        top_args = similarity.argsort()[::-1][:top_n]\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            [(raw_text_for_question[i], similarity[i]) for i in top_args], \n",
    "            columns=['text', 'similarity']\n",
    "        ).sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = QuestionClassifierWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category academics not found in the embeddings dictionary.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What You’ll Learn\\nOur 42-credit program combi...</td>\n",
       "      <td>0.727046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My name is Dr. Kathleen Peterson and I’m a Pro...</td>\n",
       "      <td>0.726161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What You’ll Learn\\nYou’ll be prepared to care ...</td>\n",
       "      <td>0.724720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Applications are now open\\nApplications for th...</td>\n",
       "      <td>0.722066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What You’ll Learn\\nThe RN-BSN Fast Track Compl...</td>\n",
       "      <td>0.708482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What You’ll Learn\\nOur 42-credit program combi...</td>\n",
       "      <td>0.692306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Learn from the Best in the Field\\nOur programs...</td>\n",
       "      <td>0.689883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The world needs more nurses. Our degrees produ...</td>\n",
       "      <td>0.688628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What You’ll Learn\\nThis part-time program is d...</td>\n",
       "      <td>0.683548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Letter to All Prospective Nursing Students for...</td>\n",
       "      <td>0.679201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  similarity\n",
       "0  What You’ll Learn\\nOur 42-credit program combi...    0.727046\n",
       "1  My name is Dr. Kathleen Peterson and I’m a Pro...    0.726161\n",
       "2  What You’ll Learn\\nYou’ll be prepared to care ...    0.724720\n",
       "3  Applications are now open\\nApplications for th...    0.722066\n",
       "4  What You’ll Learn\\nThe RN-BSN Fast Track Compl...    0.708482\n",
       "5  What You’ll Learn\\nOur 42-credit program combi...    0.692306\n",
       "6  Learn from the Best in the Field\\nOur programs...    0.689883\n",
       "7  The world needs more nurses. Our degrees produ...    0.688628\n",
       "8  What You’ll Learn\\nThis part-time program is d...    0.683548\n",
       "9  Letter to All Prospective Nursing Students for...    0.679201"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.retreive_text(\"tell me about the nursing program\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking\n",
    "\n",
    "The final step for text retrieval here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.6085,  5.7623])\n"
     ]
    }
   ],
   "source": [
    "# Reranking\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\n",
    "model.eval()\n",
    "\n",
    "pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "tensor([-5.6085,  5.7623])\n",
      "116 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
