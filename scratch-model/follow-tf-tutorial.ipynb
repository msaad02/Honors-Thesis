{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following tutorial from tensorflow\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "\n",
    "This tutorial trains a sequence to sequence (seq2seq) model for Spanish to English translation. This is supposed to be a beginner friendly tutorial, but for new people like myself, it is not completely intuitive.\n",
    "\n",
    "The most important part of this tutorial I think is to setup the data pipeline. Many of the basic RNN/Transformer implementations I've seen taken in large chunks of text instead of Q/A type data (Kaparthys examples (from his [tutorials on youtube](https://www.youtube.com/watch?v=kCc8FmEb1nY&pp=ygUVa2FwYXJ0aHkgdHJhbnNmb3JtZXJz) or generally [his github](https://github.com/karpathy)) generally do this method - and so does tf's [Generate Text with RNN tutorial](https://www.tensorflow.org/text/tutorials/text_generation), which is based off Kaparthys work). Perhaps this is the correct way to go - following how tools like GPT are built by starting with large chunks of text like this, then fine tuning them for question answering. That's not completely intuitive to me though, so I am going ot try this method. This example is all about english to spanish translation, which I expect will probably have similar IO to Q/A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My implementation\n",
    "\n",
    "I'm using the dataset generated by GPT, which contains ~113k questions and answers about SUNY Brockport. Elsewhere in this repo I published this dataset to hugginface to make it more accessible. For this notebook, I'm using that dataset from hugginface.\n",
    "\n",
    "---\n",
    "\n",
    "# Specifically this first iteration\n",
    "\n",
    "I'm using only a small portion of the dataset (using the filtererd urls from the semantic search cleaning). This subset only contains 7k question answer pairs, which allows engineering to be sped up for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an inquiery related to SUNY Brockport - from academics, admissions, and faculty support to student life. Prioritize accuracy and brevity.\"\\n\\n### Instruction:\\nWhat limitations are there for non-matriculated students if they choose to enroll at a later date?\\n\\n### Response:\\nIf non-matriculated students choose to enroll at a later date, they are subject to all changes in the course catalog.',\n",
       " 'Below is an inquiery related to SUNY Brockport - from academics, admissions, and faculty support to student life. Prioritize accuracy and brevity.\"\\n\\n### Instruction:\\nWhere can I find more information about scholarships and financial aid?\\n\\n### Response:\\nYou can find more information about scholarships and financial aid by visiting the college website or contacting the college\\'s financial aid office.',\n",
       " 'Below is an inquiery related to SUNY Brockport - from academics, admissions, and faculty support to student life. Prioritize accuracy and brevity.\"\\n\\n### Instruction:\\nWhat resources are available at the library in SUNY Brockport?\\n\\n### Response:\\nThe library at SUNY Brockport provides various resources such as books, journals, databases, and study spaces to support academic research and learning.',\n",
       " 'Below is an inquiery related to SUNY Brockport - from academics, admissions, and faculty support to student life. Prioritize accuracy and brevity.\"\\n\\n### Instruction:\\nWhat is the contact information for SUNY Brockport?\\n\\n### Response:\\nThe contact information for SUNY Brockport is 350 New Campus Drive, Brockport, NY 14420, and the phone number is (585) 395-2211.',\n",
       " 'Below is an inquiery related to SUNY Brockport - from academics, admissions, and faculty support to student life. Prioritize accuracy and brevity.\"\\n\\n### Instruction:\\nWhat is the address of SUNY Brockport?\\n\\n### Response:\\nSUNY Brockport is located at 350 New Campus Drive, Brockport, NY 14420.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I need to make a tf.data pipeline. I have a huggingface dataset, so I need to convert it somehow.\n",
    "# Huggingface has tools built in to convert to tf.data, but I don't think they're exactly what I want.\n",
    "# I think I might just try and breakdown the huggingface dataset to standard python objects to convert it all.\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # stop showing tensorflow logs...\n",
    "\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "dataset_name = \"msaad02/formatted-ss-cleaned-brockport-qa\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "dataset = dataset.to_dict()['text']\n",
    "dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting\n",
    "\n",
    "This dataset was built for the fine-tuning step. Perhaps I should change this such that the questions and answers are in separate columns, and then put it all together only in the notebook for fine-tuning. It should be simple enough to work with like this though, so for now this step is just going to split it into QA pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_output(s):\n",
    "    \"\"\"\n",
    "    Splits a string into a question and answer pair\n",
    "    \"\"\"\n",
    "    output_split = s.split('\\n\\n### Response:\\n')\n",
    "    input_split = output_split[0].split('### Instruction:\\n')[1]\n",
    "    return input_split, output_split[1]\n",
    "\n",
    "# Map above function over the list of questions, and create a list of questions and answers separately\n",
    "context_raw, target_raw = [list(t) for t in zip(*[split_input_output(string) for string in dataset])]\n",
    "\n",
    "# NOTE: Chosing to follow the naming conventions from the tutorial, for reference then:\n",
    "# CONTEXT refers to QUESTIONS, and TARGET refers to ANSWERS\n",
    "\n",
    "# \"Raw\" is used since currently the data is in string format, and needs to be standardized and vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can current students access their university email?\n",
      "Current students can access their university email through the Webmail quicklink. It provides a secure platform for students to manage their university email accounts and communicate with faculty, staff, and peers.\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(context_raw[-1])\n",
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a tf.data dataset\n",
    "\n",
    "Now my data matches the format of the Spanish-to-English example in the tutorial, so I can use the same code to create a tf.data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train_mask = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "train_context = np.array(context_raw)[is_train_mask]\n",
    "train_target = np.array(target_raw)[is_train_mask]\n",
    "\n",
    "val_context = np.array(context_raw)[~is_train_mask]\n",
    "val_target = np.array(target_raw)[~is_train_mask]\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((train_context, train_target))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((val_context, val_target))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Who can I contact for more information about the application process at SUNY Brockport?'\n",
      " b'What majors are offered in the College of Arts and Sciences at SUNY Brockport?'\n",
      " b'What specialized programs are offered at SUNY Brockport?'\n",
      " b\"Which states' flagship universities are eligible for the tuition match program?\"\n",
      " b'Are there IT support services available for students at SUNY Brockport?'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'You can contact the Office of Undergraduate Admissions at (585) 395-2211 for more information.'\n",
      " b'Some of the majors offered in the College of Arts and Sciences at SUNY Brockport include Biology, Chemistry, English (Creative Writing and Literature), History, Mathematics, and Psychology.'\n",
      " b'SUNY Brockport offers various specialized programs, which can be found on the college website.'\n",
      " b'The states eligible for the flagship tuition match program at SUNY Brockport are Connecticut, Pennsylvania, New Jersey, Massachusetts, Vermont, New Hampshire, Illinois, and California.'\n",
      " b'Yes, there are IT support services available for students at SUNY Brockport. You can contact their IT Service Desk for assistance.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Check to see it worked\n",
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "    print(example_context_strings[:5])\n",
    "    print()\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Standardization\n",
    "\n",
    "The text vectorization method from keras will take care of a lot of this, but this is an additional step with some specifics to Spanish, but also adds special tokens for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    ragged=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '?', 'the', 'what', 'brockport']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', ',', '.', 'the', '[START]', '[END]', 'and']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 40, 13, 15, 36, 9, 63, 22, 32, 5, 106, 97, 12, 8, 7, 4, 3],\n",
       " [2, 6, 59, 11, 56, 19, 5, 51, 14, 69, 18, 149, 12, 8, 7, 4, 3],\n",
       " [2, 6, 385, 37, 11, 56, 12, 8, 7, 4, 3]]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] who can i contact for more information about the application process at suny brockport ? [END]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens.to_tensor())\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens.to_tensor() != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process The Dataset\n",
    "\n",
    "*DESCRIPTION TAKEN DIRECTLY FROM THE TENSORFLOW TUTORIAL*\n",
    "\n",
    "The process_text function below converts the Datasets of strings, into 0-padded tensors of token IDs. It also converts from a (context, target) pair to an ((context, target_in), target_out) pair for training with keras.Model.fit. Keras expects (inputs, labels) pairs, the inputs are the (context, target_in) and the labels are target_out. The difference between target_in and target_out is that they are shifted by one step relative to eachother, so that at each location the label is the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = context_text_processor(context).to_tensor()\n",
    "    target = target_text_processor(target)\n",
    "    targ_in = target[:,:-1].to_tensor()\n",
    "    targ_out = target[:,1:].to_tensor()\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   17   13   16  107   26 1344  539 2504    4]\n",
      "\n",
      "[  5  16  13 893 213  52 609   1 128  10]\n",
      "[ 16  13 893 213  52 609   1 128  10 128]\n"
     ]
    }
   ],
   "source": [
    "# Here is the first sequence of each, from the first batch:\n",
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "  print(ex_context_tok[0, :10].numpy()) \n",
    "  print()\n",
    "  print(ex_tar_in[0, :10].numpy()) \n",
    "  print(ex_tar_out[0, :10].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FROM TUTORIAL\n",
    "# This tutorial uses a lot of low level API's where it's easy to get shapes wrong. \n",
    "# This class is used to check shapes throughout the tutorial.\n",
    "\n",
    "#@title\n",
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "    for name, new_dim in parsed.items():\n",
    "      old_dim = self.shapes.get(name, None)\n",
    "      \n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder/Decoder Model\n",
    "\n",
    "Just following tutorial at this point, I'll go back to learn it all once (and if) I get it working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                                    mask_zero=True)\n",
    "\n",
    "        # The RNN layer processes those vectors sequentially.\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='sum',\n",
    "            layer=tf.keras.layers.GRU(units,\n",
    "                                # Return the sequence and state\n",
    "                                return_sequences=True,\n",
    "                                recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    def call(self, x):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(x, 'batch s')\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding vector for each token.\n",
    "        x = self.embedding(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 3. The GRU processes the sequence of embeddings.\n",
    "        x = self.rnn(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 4. Returns the new sequence of embeddings.\n",
    "        return x\n",
    "\n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "        context = self.text_processor(texts).to_tensor()\n",
    "        context = self(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (64, 22)\n",
      "Encoder output, shape (batch, s, units): (64, 22, 256)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "ex_context = encoder(ex_context_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, x, context):\n",
    "    shape_checker = ShapeChecker()\n",
    "\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(attn_scores, 'batch heads t s')\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "    shape_checker(attn_scores, 'batch t s')\n",
    "    self.last_attention_weights = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (64, 22, 256)\n",
      "Target sequence, shape (batch, t, units): (64, 65, 256)\n",
      "Attention result, shape (batch, t, units): (64, 65, 256)\n",
      "Attention weights, shape (batch, t, s):    (64, 65, 22)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)\n",
    "ex_tar_embed = embed(ex_tar_in)\n",
    "\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 1.        , 1.        , 0.9999999 , 1.        ,\n",
       "       0.9999999 , 0.99999994, 1.        , 1.        , 0.9999999 ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.99999994, 0.9999999 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tutorial) Here are the attention weights across the context sequences at t=0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_weights = attention_layer.last_attention_weights\n",
    "mask=(ex_context_tok != 0).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(mask)\n",
    "plt.title('Mask');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.word_to_id = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]')\n",
    "    self.id_to_word = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]',\n",
    "        invert=True)\n",
    "    self.start_token = self.word_to_id('[START]')\n",
    "    self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "    self.units = units\n",
    "\n",
    "\n",
    "    # 1. The embedding layer converts token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                               units, mask_zero=True)\n",
    "\n",
    "    # 2. The RNN keeps track of what's been generated so far.\n",
    "    self.rnn = tf.keras.layers.GRU(units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = CrossAttention(units)\n",
    "\n",
    "    # 4. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "  shape_checker = ShapeChecker()\n",
    "  shape_checker(x, 'batch t')\n",
    "  shape_checker(context, 'batch s units')\n",
    "\n",
    "  # 1. Lookup the embeddings\n",
    "  x = self.embedding(x)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 2. Process the target sequence.\n",
    "  x, state = self.rnn(x, initial_state=state)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 3. Use the RNN output as the query for the attention over the context.\n",
    "  x = self.attention(x, context)\n",
    "  self.last_attention_weights = self.attention.last_attention_weights\n",
    "  shape_checker(x, 'batch t units')\n",
    "  shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "  # Step 4. Generate logit predictions for the next token.\n",
    "  logits = self.output_layer(x)\n",
    "  shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "  if return_state:\n",
    "    return logits, state\n",
    "  else:\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(target_text_processor, UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (batch, s, units) (64, 22, 256)\n",
      "input target tokens shape: (batch, t) (64, 65)\n",
      "logits shape shape: (batch, target_vocabulary_size) (64, 65, 5000)\n"
     ]
    }
   ],
   "source": [
    "logits = decoder(ex_context, ex_tar_in)\n",
    "\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "  batch_size = tf.shape(context)[0]\n",
    "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "  embedded = self.embedding(start_tokens)\n",
    "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
    "\n",
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "  words = self.id_to_word(tokens)\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "  return result\n",
    "\n",
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "  logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "\n",
    "  if temperature == 0.0:\n",
    "    next_token = tf.argmax(logits, axis=-1)\n",
    "  else:\n",
    "    logits = logits[:, -1, :]/temperature\n",
    "    next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "  # If a sequence produces an `end_token`, set it `done`\n",
    "  done = done | (next_token == self.end_token)\n",
    "  # Once a sequence is done it only produces 0-padding.\n",
    "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "  return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'ams should jurors register refining wages interests epidemiology understand before',\n",
       "       b'minute discrimination mentored housing awards college could excess digital requests',\n",
       "       b'equal larger equality tentative signup geology debate war force off'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the loop variables.\n",
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "  # Run one step.\n",
    "  next_token, done, state = decoder.get_next_token(\n",
    "      ex_context, next_token, done, state, temperature=1.0)\n",
    "  # Add the token to the output.\n",
    "  tokens.append(next_token)\n",
    "\n",
    "# Stack all the tokens together.\n",
    "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
    "\n",
    "# Convert the tokens back to a a string\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, units,\n",
    "               context_text_processor,\n",
    "               target_text_processor):\n",
    "    super().__init__()\n",
    "    # Build the encoder and decoder\n",
    "    encoder = Encoder(context_text_processor, units)\n",
    "    decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call(self, inputs):\n",
    "    context, x = inputs\n",
    "    context = self.encoder(context)\n",
    "    logits = self.decoder(context, x)\n",
    "\n",
    "    #TODO(b/250038731): remove this\n",
    "    try:\n",
    "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape: (batch, s, units) (64, 22)\n",
      "Target tokens, shape: (batch, t) (64, 65)\n",
      "logits, shape: (batch, t, target_vocabulary_size) (64, 65, 5000)\n"
     ]
    }
   ],
   "source": [
    "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "logits = model((ex_context_tok, ex_tar_in))\n",
    "\n",
    "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
    "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_loss': 8.517193, 'expected_acc': 0.0002}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
    " \"expected_acc\": 1/vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 31s 1s/step - loss: 8.5295 - masked_acc: 3.0414e-05 - masked_loss: 8.5295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.529455184936523,\n",
       " 'masked_acc': 3.0413624699576758e-05,\n",
       " 'masked_loss': 8.529455184936523}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 10/100 [==>...........................] - ETA: 3:27 - loss: 7.4177 - masked_acc: 0.0627 - masked_loss: 7.4177"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     train_ds\u001b[39m.\u001b[39;49mrepeat(), \n\u001b[1;32m      3\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     steps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[1;32m      6\u001b[0m     validation_steps \u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      8\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(patience\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=100,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now using the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts, *,\n",
    "              max_length=50,\n",
    "              temperature=0.0):\n",
    "  # Process the input texts\n",
    "  context = self.encoder.convert_input(texts)\n",
    "  batch_size = tf.shape(texts)[0]\n",
    "\n",
    "  # Setup the loop inputs\n",
    "  tokens = []\n",
    "  attention_weights = []\n",
    "  next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    # Generate the next token\n",
    "    next_token, done, state = self.decoder.get_next_token(\n",
    "        context, next_token, done,  state, temperature)\n",
    "\n",
    "    # Collect the generated tokens\n",
    "    tokens.append(next_token)\n",
    "    attention_weights.append(self.decoder.last_attention_weights)\n",
    "\n",
    "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Stack the lists of tokens and attention weights.\n",
    "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "  result = self.decoder.tokens_to_text(tokens)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Translator' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtranslate([\u001b[39m'\u001b[39m\u001b[39m¿Todavía está en casa?\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m# Are you still home\u001b[39;00m\n\u001b[1;32m      2\u001b[0m result[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mdecode()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Translator' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "result = model.translate(['¿Todavía está en casa?']) # Are you still home\n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Translator' object has no attribute 'plot_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mplot_attention(\u001b[39m'\u001b[39m\u001b[39m¿Todavía está en casa?\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Are you still home\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Translator' object has no attribute 'plot_attention'"
     ]
    }
   ],
   "source": [
    "model.plot_attention('¿Todavía está en casa?') # Are you still home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is my life.\n",
    "model.plot_attention('Esta es mi vida.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "  def translate(self, inputs):\n",
    "    return self.model.translate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = export.translate(tf.constant(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = export.translate(tf.constant(inputs))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_28155/1255225850.py\", line 7, in translate  *\n        return self.model.translate(inputs)\n\n    AttributeError: 'Translator' object has no attribute 'translate'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py:1240\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m metrics\u001b[39m.\u001b[39mIncrementWriteApi(_SAVE_V2_LABEL)\n\u001b[0;32m-> 1240\u001b[0m save_and_return_nodes(obj, export_dir, signatures, options)\n\u001b[1;32m   1242\u001b[0m metrics\u001b[39m.\u001b[39mIncrementWrite(write_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py:1276\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1272\u001b[0m saved_model \u001b[39m=\u001b[39m saved_model_pb2\u001b[39m.\u001b[39mSavedModel()\n\u001b[1;32m   1273\u001b[0m meta_graph_def \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39mmeta_graphs\u001b[39m.\u001b[39madd()\n\u001b[1;32m   1275\u001b[0m _, exported_graph, object_saver, asset_info, saved_nodes, node_paths \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1276\u001b[0m     _build_meta_graph(obj, signatures, options, meta_graph_def))\n\u001b[1;32m   1277\u001b[0m saved_model\u001b[39m.\u001b[39msaved_model_schema_version \u001b[39m=\u001b[39m (\n\u001b[1;32m   1278\u001b[0m     constants\u001b[39m.\u001b[39mSAVED_MODEL_SCHEMA_VERSION)\n\u001b[1;32m   1280\u001b[0m \u001b[39m# Write the checkpoint, copy assets into the assets directory, and write out\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# the SavedModel proto itself.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py:1455\u001b[0m, in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a MetaGraph under a save context.\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m \n\u001b[1;32m   1430\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[39m  saveable_view.node_paths: _SaveableView paths.\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[39mwith\u001b[39;00m save_context\u001b[39m.\u001b[39msave_context(options):\n\u001b[0;32m-> 1455\u001b[0m   \u001b[39mreturn\u001b[39;00m _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py:1402\u001b[0m, in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[39mif\u001b[39;00m signatures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m   signatures \u001b[39m=\u001b[39m signature_serialization\u001b[39m.\u001b[39mfind_function_to_export(\n\u001b[1;32m   1399\u001b[0m       augmented_graph_view)\n\u001b[1;32m   1401\u001b[0m signatures, wrapped_functions \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1402\u001b[0m     signature_serialization\u001b[39m.\u001b[39;49mcanonicalize_signatures(signatures))\n\u001b[1;32m   1403\u001b[0m signature_serialization\u001b[39m.\u001b[39mvalidate_augmented_graph_view(augmented_graph_view)\n\u001b[1;32m   1404\u001b[0m signature_map \u001b[39m=\u001b[39m signature_serialization\u001b[39m.\u001b[39mcreate_signature_map(signatures)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/saved_model/signature_serialization.py:131\u001b[0m, in \u001b[0;36mcanonicalize_signatures\u001b[0;34m(signatures)\u001b[0m\n\u001b[1;32m    129\u001b[0m wrapped_functions \u001b[39m=\u001b[39m {}\n\u001b[1;32m    130\u001b[0m \u001b[39mfor\u001b[39;00m signature_key, function \u001b[39min\u001b[39;00m signatures\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 131\u001b[0m   original_function \u001b[39m=\u001b[39m signature_function \u001b[39m=\u001b[39m _get_signature(function)\n\u001b[1;32m    132\u001b[0m   \u001b[39mif\u001b[39;00m signature_function \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected a TensorFlow function for which to generate a signature, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mfunction\u001b[39m}\u001b[39;00m\u001b[39m. Only `tf.functions` with an input signature or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconcrete functions can be used as a signature.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/saved_model/signature_serialization.py:43\u001b[0m, in \u001b[0;36m_get_signature\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_signature\u001b[39m(function):\n\u001b[1;32m     41\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(function, (defun\u001b[39m.\u001b[39mFunction, def_function\u001b[39m.\u001b[39mFunction)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m     42\u001b[0m       function\u001b[39m.\u001b[39minput_signature \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 43\u001b[0m     function \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     44\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(function, defun\u001b[39m.\u001b[39mConcreteFunction):\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1238\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1238\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m   1239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m   1242\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    761\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    762\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    764\u001b[0m     \u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    765\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    768\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 171\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:166\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    164\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:396\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m   args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    394\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 396\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    397\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    399\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_function_captures  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:300\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    299\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 300\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    301\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    303\u001b[0m         args,\n\u001b[1;32m    304\u001b[0m         kwargs,\n\u001b[1;32m    305\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    306\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    307\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    308\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    309\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    310\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    311\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    313\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    314\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    319\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1214\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1214\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:667\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    664\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    665\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    666\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 667\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    668\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:484\u001b[0m, in \u001b[0;36mclass_method_to_instance_method.<locals>.bound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapped_fn(weak_instance(), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    481\u001b[0m \u001b[39m# If __wrapped__ was replaced, then it is always an unbound function.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m# However, the replacer is still responsible for attaching self properly.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39m# TODO(mdan): Is it possible to do it here instead?\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1200\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1200\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1202\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1189\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[1;32m   1190\u001b[0m       original_func,\n\u001b[1;32m   1191\u001b[0m       args,\n\u001b[1;32m   1192\u001b[0m       kwargs,\n\u001b[1;32m   1193\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[1;32m   1194\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1195\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[1;32m   1196\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1197\u001b[0m       ))\n\u001b[1;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filewvz5xy1_.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__translate\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtranslate, (ag__\u001b[39m.\u001b[39mld(inputs),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_28155/1255225850.py\", line 7, in translate  *\n        return self.model.translate(inputs)\n\n    AttributeError: 'Translator' object has no attribute 'translate'\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.saved_model.save(export, 'translator',\n",
    "                    signatures={'serving_default': export.translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reloaded = tf.saved_model.load('translator')\n",
    "_ = reloaded.translate(tf.constant(inputs)) #warmup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
