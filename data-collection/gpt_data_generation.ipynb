{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own attempt at parallelizing OpenAI API requests\n",
    "\n",
    "Found this after I made an earlier version of this: https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py \n",
    "\n",
    "Requires data in jsonl format, which is inconveinient at this time. Might move to this in the future to take advantage of being able to explicitly set rate limits and abide by those rules. In this implementation, I'm eyeballing the rate/token limits. Trying to operate in 50-75% of each territory as to not get messed up results... ymmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from itertools import chain\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False # sample or run on full dataset?\n",
    "sample_size = 4 # if test=True\n",
    "\n",
    "scape_output = pickle.load(open('data/scraper_output.p', 'rb'))\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if test: \n",
    "    keys = random.sample(list(scape_output.keys()), sample_size)\n",
    "    scape_output = {key: scape_output[key] for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(key, value, question_count):\n",
    "    # record the time before the request is sent\n",
    "    start_time = time.time()\n",
    "\n",
    "    soup = BeautifulSoup(value.content, \"html.parser\")\n",
    "    cleaned = re.sub('[\\n]+', '\\n', soup.text.strip())\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the cleaned HTML given below, generate as many questions possible with their answers.\n",
    "    Try to make the questions relevant from the perspective of a prospective or current student, as well as faculty and staff.\n",
    "    Format your responce in JSON, with the \"instruction\" field containing the question, an empty \"input\" field, and the answer in the \"output\" field.\n",
    "    Include up to {question_count} questions, each being a sentence or two long in length. Do not include question number.\n",
    "    Keep answers somewhat brief, but be enthusiastic in your response!\\n\\n\"\"\"\n",
    "    \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful question answer generator.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\\n This is the cleaned HTML: \\n{cleaned}\\n Start:.\"},\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    # calculate the time it took to receive the response\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    qa = completion.choices[0].message.content\n",
    "    tokens = completion.usage[\"total_tokens\"]\n",
    "\n",
    "    print(f\"Success! Complete in {response_time:.2f}s with {tokens} tokens for {key}\")\n",
    "\n",
    "    return key, qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output = {}\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=64) as executor:\n",
    "    future_to_item = {executor.submit(process_item, key, value, 25): key for key, value in scape_output.items()}\n",
    "    for future in concurrent.futures.as_completed(future_to_item):\n",
    "        key = future_to_item[future]\n",
    "        try:\n",
    "            key, qa = future.result()\n",
    "            gpt_output[key] = qa\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (key, exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gpt_output, open('data/gpt_output.p', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse GPT output to JSON\n",
    "\n",
    "While the prompt above to translate clean HTML to question/answer format does specify to do it in JSON format, GPT3.5 does not always do it perfectly. However, it always get close. Instead of trying to fix the JSON output from GPT, In this step I'm using regex to parse for all the instructions (questions), and outputs (answers). This is returned into a python list of dictionaries, which is appended for each webpage. Eventually I shuffle this so the questions are all mixed up instead of grouped by webpage, and dump it to a json file.\n",
    "\n",
    "For any questions which seem off, investigate the original webpage. I've left gpt_output as a python dictionary specifically for this reason, so that we can always refer back to the data and know exactly where it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output = pickle.load(open('data/gpt_output.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json(gpt_output: dict, filename: str) -> None:\n",
    "    # The regular expression pattern for a JSON object with \"instruction\" and \"output\"\n",
    "    pattern = r'\"instruction\":\\s*\"(.*?)\",.*?\"output\":\\s*\"(.*?)\"'\n",
    "\n",
    "    def extract_data(s):\n",
    "        matches = re.findall(pattern, s, flags=re.DOTALL)\n",
    "        # Add a conditional filter in the list comprehension\n",
    "        data = [{\"instruction\": m[0], \"output\": m[1]} for m in matches if m[0] and m[1] and '\"' not in m[0] and '\"' not in m[1]]\n",
    "        return data\n",
    "\n",
    "    jsonqa = []\n",
    "\n",
    "    for key, value in gpt_output.items():\n",
    "        clean_value = extract_data(value)\n",
    "        jsonqa.append(clean_value)\n",
    "\n",
    "    jsonqa = list(chain(*jsonqa))\n",
    "\n",
    "    random.shuffle(jsonqa)\n",
    "\n",
    "    # Write to a JSON file\n",
    "    with open('data/' + filename + '.json', 'w') as f:\n",
    "        json.dump(jsonqa, f, indent=4)  # Dump the entire list at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_json(gpt_output, \"full_dataset_v4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Dataset\n",
    "\n",
    "Since training will take a while on the full dataset, we can pick out a fraction of the data to train on first. This is helpful for mocking up the end state, and experimenting with how different data will react to training. In this case, I am filtering to all urls which contain \".edu/admissions/\" (basically just direct derivatives of admissions page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to 117 webpages (2.21% of full dataset)\n"
     ]
    }
   ],
   "source": [
    "filtered_urls = [url for url in gpt_output.keys() if '.edu/admissions/' in url]\n",
    "print(\"Filtering to\", len(filtered_urls), \"webpages (\" + str(round(len(filtered_urls)/ len(gpt_output) * 100, 2)) + \"% of full dataset)\")\n",
    "\n",
    "filtered_dict = {link : gpt_output[link] for link in filtered_urls}\n",
    "\n",
    "generate_json(filtered_dict, \"admissions_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
