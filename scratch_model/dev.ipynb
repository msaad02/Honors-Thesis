{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Epoch 1/2\n",
      "145/145 [==============================] - 78s 365ms/step - loss: 7.5146 - masked_accuracy: 0.0469 - val_loss: 6.6366 - val_masked_accuracy: 0.0861\n",
      "Epoch 2/2\n",
      "145/145 [==============================] - 48s 329ms/step - loss: 5.9121 - masked_accuracy: 0.1511 - val_loss: 5.2352 - val_masked_accuracy: 0.2034\n",
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  65579008  \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  115993600 \n",
      "                                                                 \n",
      " dense_24 (Dense)            multiple                  2565000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184,137,608\n",
      "Trainable params: 184,137,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Transformer Model\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "This is again following the tutorial on tensorflow's website. Did a lot of digging to find \n",
    "simpler implementations but it seems breaking things down in the following way is actually \n",
    "the most popular way to do it. Things may be better anyway since breaking it down makes \n",
    "learning easier. \n",
    "\n",
    "I plan to go slowly through this part and spend about a day with it to make sure I understand \n",
    "it. However, it's worth mentioning that this should be plug and play based on the same data \n",
    "pipeline I defined in `tf_dataset.py`. Note that the grand majority of this code is copy-pasted \n",
    "from the tutorial. I did have to make some departures from the tutorial, however, since it\n",
    "was written for a different use case with different data. This is mostly in the export/running\n",
    "the thing. My tokenizer was different, which caused some issues.\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: Installing tensorflow_datasets and tensorflow_text updated tf to 2.13.0 from 12.2.1, if it breaks revert and find correct versions\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # stop showing tensorflow logs...\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scratch_model.dataset import get_datasets\n",
    "\n",
    "# Check GPU is being used. Prints [] if not\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "\n",
    "# Prevent tensorflow from allocating all GPU memory at once\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True) # Nice!\n",
    "\n",
    "# MODEL PARAMS: These are roughly the same parameters as used in the original transformer paper.\n",
    "BATCH_SIZE = 64        \n",
    "EPOCHS = 2             # What we used for transformer_v1\n",
    "NUM_LAYERS = 6          # 4 \n",
    "D_MODEL = 512           # 128\n",
    "DFF = 2048              # 512\n",
    "NUM_HEADS = 8           # 8\n",
    "DROPOUT_RATE = 0.1      # 0.1\n",
    "\n",
    "save_dir = './models/transformer_v3.01'\n",
    "\n",
    "train_ds, val_ds, text_processor = get_datasets(batch_size = BATCH_SIZE)\n",
    "\n",
    "\n",
    "# ---- Defining stuff ----\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "  \n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model),\n",
    "        tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        dff=dff,\n",
    "                        dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                *,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dff,\n",
    "                dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "                input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                            num_heads=num_heads, dff=dff,\n",
    "                            vocab_size=input_vocab_size,\n",
    "                            dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                            num_heads=num_heads, dff=dff,\n",
    "                            vocab_size=target_vocab_size,\n",
    "                            dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x  = inputs\n",
    "\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits\n",
    "  \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# ---- Setup loss and metrics ----\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "# ---- Training ----\n",
    "transformer = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    input_vocab_size=5000,  # This is the vocab size used for all the datasets.\n",
    "    target_vocab_size=5000,\n",
    "    dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.98,\n",
    "    epsilon=1e-9\n",
    ")\n",
    "\n",
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "transformer.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds)\n",
    "\n",
    "print(transformer.summary())\n",
    "\n",
    "# ---- Save model ----\n",
    "\n",
    "vocab = text_processor.get_vocabulary()\n",
    "\n",
    "MAX_TOKENS = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Transformer at 0x7ff0f2f2aa10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training ----\n",
    "transformer2 = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    input_vocab_size=5000,  # This is the vocab size used for all the datasets.\n",
    "    target_vocab_size=5000,\n",
    "    dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Export model parameters\n",
    "params = {\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'd_model': D_MODEL,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dff': DFF,\n",
    "    'input_vocab_size': 5000,\n",
    "    'target_vocab_size': 5000,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "    'max_tokens': MAX_TOKENS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, 'params.json'), 'w') as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./models/transformer_v3.02/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#transformer.save(save_dir)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# or, if the transformer is a Keras model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39msave_weights(save_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "#transformer.save(save_dir)\n",
    "# or, if the transformer is a Keras model\n",
    "transformer.save_weights(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchModel(tf.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer,\n",
    "        text_processor: tf.keras.layers.TextVectorization,\n",
    "        vocab\n",
    "    ):\n",
    "        self.transformer = transformer\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab = vocab\n",
    "        self.vocab_tf = tf.constant(self.vocab)\n",
    "\n",
    "    def _predict_next(self, question, output_array, i):\n",
    "        \"Predicts the next token given the question and the output array\"\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        prediction = transformer([question, output], training=False)\n",
    "        prediction = prediction[:, -1:, :]\n",
    "        prediction_id = tf.argmax(prediction, axis=-1)\n",
    "        output_array = output_array.write(i+1, prediction_id[0])\n",
    "        output = tf.transpose(output_array.stack())\n",
    "\n",
    "        text = tf.strings.reduce_join(\n",
    "            tf.map_fn(lambda x: self.vocab_tf[x], tf.squeeze(output), dtype=tf.string), separator=\" \"\n",
    "        )\n",
    "\n",
    "        return prediction_id, text, output_array\n",
    "    \n",
    "    def _stream_result(self, question, output_array, max_tokens, end):\n",
    "        \"Streams the result of the prediction\"\n",
    "        for i in tf.range(max_tokens):\n",
    "                prediction_id, text, output_array = self._predict_next(question, output_array, i)\n",
    "    \n",
    "                if prediction_id == end:\n",
    "                    break\n",
    "\n",
    "                yield text\n",
    "\n",
    "    def _return_result(self, question, output_array, max_tokens, end):\n",
    "        \"Returns the result of the prediction\"\n",
    "        for i in tf.range(max_tokens):\n",
    "                prediction_id, text, output_array = self._predict_next(question, output_array, i)\n",
    "    \n",
    "                if prediction_id == end:\n",
    "                    break\n",
    "    \n",
    "        return text\n",
    "\n",
    "    def __call__(self, question: str, max_tokens: int = 256, stream: bool = False):\n",
    "        \"Oversees the prediction process. Returns a generator if stream=True\"\n",
    "        question = tf.convert_to_tensor([question])\n",
    "        question = self.text_processor(question).to_tensor()\n",
    "\n",
    "        start_end = text_processor([''])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        if stream:\n",
    "            return self._stream_result(question, output_array, max_tokens, end)\n",
    "        else:\n",
    "            return self._return_result(question, output_array, max_tokens, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScratchModel(transformer, text_processor, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what does export function do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[START] you can find the [UNK] , you can be a'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"How can I apply?\", stream=False, max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[START] you'\n",
      "b'[START] you can'\n",
      "b'[START] you can find'\n",
      "b'[START] you can find the'\n",
      "b'[START] you can find the [UNK]'\n",
      "b'[START] you can find the [UNK] ,'\n",
      "b'[START] you can find the [UNK] , you'\n",
      "b'[START] you can find the [UNK] , you can'\n",
      "b'[START] you can find the [UNK] , you can be'\n",
      "b'[START] you can find the [UNK] , you can be a'\n"
     ]
    }
   ],
   "source": [
    "generated_texts = model(question=\"How can I apply?\", max_tokens=10, stream=True)\n",
    "\n",
    "for text in generated_texts:  # Convert the scalar tensor to a Python scalar\n",
    "    print(text.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
