{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.tokenizer(\n",
    "            '[START] ' + self.questions[idx] + ' [END]',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        answer = self.tokenizer(\n",
    "            '[START] ' + self.answers[idx] + ' [END]',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return question['input_ids'].squeeze(0), answer['input_ids'].squeeze(0)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    questions, answers = zip(*batch)\n",
    "    questions = pad_sequence(questions, batch_first=True, padding_value=0)\n",
    "    answers = pad_sequence(answers, batch_first=True, padding_value=0)\n",
    "    return questions, answers[:, :-1], answers[:, 1:]\n",
    "\n",
    "def get_datasets(tokenizer, batch_size=64):\n",
    "    dataset = load_dataset(\"msaad02/brockport-gpt-4-qa\")\n",
    "    dataset = dataset['train'].to_pandas()\n",
    "\n",
    "    context_raw = dataset['question'].to_list()\n",
    "    target_raw = dataset['answer'].to_list()\n",
    "\n",
    "    is_train_mask = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "    train_context = np.array(context_raw)[is_train_mask]\n",
    "    train_target = np.array(target_raw)[is_train_mask]\n",
    "\n",
    "    val_context = np.array(context_raw)[~is_train_mask]\n",
    "    val_target = np.array(target_raw)[~is_train_mask]\n",
    "\n",
    "    train_dataset = Seq2SeqDataset(train_context, train_target, tokenizer)\n",
    "    val_dataset = Seq2SeqDataset(val_context, val_target, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# Define tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_loader, val_loader = get_datasets(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'how', 'can', 'i', 'aplpy', '?']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hi, how can I aplpy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_basic_english_normalize() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[START] \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi, how can I aplpy?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m [END]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: _basic_english_normalize() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "source": [
    "tokenizer(\n",
    "    '[START] ' + \"Hi, how can I aplpy?\" + ' [END]',\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_basic_english_normalize() got an unexpected keyword argument 'truncation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[79], line 19\u001b[0m, in \u001b[0;36mSeq2SeqDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 19\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[START] \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m [END]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START] \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswers[idx] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m [END]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m question[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), answer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: _basic_english_normalize() got an unexpected keyword argument 'truncation'"
     ]
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts tensor([[  20,  235,   32,  ...,    1,    1,    1],\n",
      "        [  30,  951,  590,  ...,    1,    1,    1],\n",
      "        [ 325,   18,    7,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [ 325, 3452,   14,  ...,    1,    1,    1],\n",
      "        [ 325,  216,  365,  ...,    1,    1,    1],\n",
      "        [ 325,   30,    7,  ...,    1,    1,    1]], device='cuda:0')\n",
      "targets tensor([[  68,   16,   17,  ...,    1,    1,    1],\n",
      "        [ 209,    4,  951,  ...,    1,    1,    1],\n",
      "        [  17,  306,  365,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [  17,  102,   14,  ...,    1,    1,    1],\n",
      "        [ 261,    6,   25,  ...,    1,    1,    1],\n",
      "        [  37,   11, 1513,  ...,    1,    1,    1]], device='cuda:0')\n",
      "tensor([[[-1.5761,  0.0794, -0.7892,  ...,  1.2343,  0.1028,  1.0264],\n",
      "         [-1.0959, -0.9691, -0.5410,  ...,  1.5932,  1.0933,  0.6204],\n",
      "         [-1.1620, -0.1358, -1.1375,  ...,  1.3729,  0.2238,  0.7762],\n",
      "         ...,\n",
      "         [-1.3205, -0.5732, -0.2531,  ...,  1.3521, -0.1316,  0.4436],\n",
      "         [-1.1432, -0.8113, -0.4033,  ...,  1.1725,  0.4256,  1.3037],\n",
      "         [-1.0262, -0.8170, -0.4263,  ...,  0.8703, -0.8826,  1.2033]],\n",
      "\n",
      "        [[-1.7391, -0.6878, -0.3690,  ...,  1.2630, -0.4196,  1.4266],\n",
      "         [-2.1855, -0.3908,  0.0648,  ...,  0.4268, -0.5184,  1.1873],\n",
      "         [-1.8576, -0.5802, -0.4439,  ...,  0.3800, -0.4872,  1.1387],\n",
      "         ...,\n",
      "         [-2.1845, -0.6172, -0.2846,  ...,  0.8450,  0.3133,  1.6040],\n",
      "         [-1.4923,  0.2517, -0.4303,  ...,  1.0215, -0.4721,  1.2303],\n",
      "         [-0.5608, -0.8784, -0.7736,  ...,  1.3139, -0.7604,  1.4515]],\n",
      "\n",
      "        [[-1.6648, -0.3316, -0.2146,  ...,  1.0434, -0.1537,  0.4781],\n",
      "         [-1.0899, -0.1589, -0.3380,  ...,  1.1788, -0.6468,  0.8203],\n",
      "         [-1.3898, -0.3534, -0.0425,  ...,  1.0320, -0.5783,  0.4205],\n",
      "         ...,\n",
      "         [-1.5825,  0.1759, -0.3197,  ...,  0.7827, -0.1668,  1.0612],\n",
      "         [-1.0953, -0.5134, -0.1687,  ...,  0.4650, -0.1270,  0.5628],\n",
      "         [-1.8780, -0.2279, -0.8894,  ...,  1.4305, -0.6618,  0.6292]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.0157, -0.6588, -0.7367,  ...,  0.5270, -0.3632,  1.1312],\n",
      "         [-2.0582,  0.1515, -0.1113,  ...,  0.4824, -0.5958,  0.5607],\n",
      "         [-1.8127,  0.4541, -0.2459,  ...,  0.6283,  0.0863,  1.0475],\n",
      "         ...,\n",
      "         [-1.8262, -0.2804, -0.0417,  ...,  0.2895, -0.6800,  0.0392],\n",
      "         [-1.7035,  0.3219, -0.4998,  ...,  0.6130, -0.2697,  1.0919],\n",
      "         [-1.7122, -0.4377,  0.0180,  ...,  0.7849, -0.8494,  1.4985]],\n",
      "\n",
      "        [[-1.5874,  0.4112, -0.0071,  ...,  0.6213, -0.1196,  0.8005],\n",
      "         [-1.2198,  0.0410,  0.2020,  ...,  0.2955, -0.2427, -0.0149],\n",
      "         [-1.5347,  0.1900, -0.0419,  ...,  0.2155, -0.8557,  0.7426],\n",
      "         ...,\n",
      "         [-1.8942, -0.3291, -0.7805,  ...,  1.0177, -0.5153, -0.5260],\n",
      "         [-1.0821, -0.4386, -0.6204,  ...,  0.7108, -0.5375,  0.6119],\n",
      "         [-1.1776, -0.2224,  0.2346,  ...,  0.8254, -1.1185,  0.5626]],\n",
      "\n",
      "        [[ 0.2080, -0.1545, -0.1414,  ..., -0.7975, -0.9284,  0.5716],\n",
      "         [-1.0082,  0.8583, -0.0379,  ..., -1.1273, -0.8648,  0.4861],\n",
      "         [-0.6894, -0.2896, -1.3066,  ...,  0.0091, -1.2670,  0.7924],\n",
      "         ...,\n",
      "         [-0.3399, -1.1361, -0.9885,  ..., -0.6198, -0.7246,  0.5179],\n",
      "         [-1.3735, -0.5739, -0.6352,  ..., -0.7174, -1.6011,  1.0053],\n",
      "         [-1.1282, -0.7129, -0.1261,  ..., -0.5483, -0.9940,  0.6535]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 512], got [64, 111]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# # Compute the loss\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# loss = loss_fn(output.reshape(-1, output.size(-1)), targets[1:, :].reshape(-1)) # Shifted targets for computing loss\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    181\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 512], got [64, 111]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For some reason, most likely due to how the model is implemented, the ScratchModel\n",
    "runs EXCEPTIONALLY slow. Again, I'm not sure why exactly, but the scratch model is\n",
    "a small transformer model that should not be running this slow. This script aims to\n",
    "re-implement the  ScratchModel in PyTorch using their optimized Transformer module\n",
    "and see if that fixes the issue.\n",
    "\n",
    "All the parameters will be the same, so we can just copy them over. The only thing\n",
    "that will change is the model implementation and data loading.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # prevent tensorflow logs\n",
    "\n",
    "# # Set path to parent directory so we can import from other folders.\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from typing import List, Tuple\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------- Model ------------------------------------------------\n",
    "EPOCHS = 13\n",
    "BATCH_SIZE = 64\n",
    "NUM_LAYERS = 6\n",
    "D_MODEL = 512\n",
    "DFF = 2048\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "\n",
    "# -------- Data ------------------------------------------------\n",
    "\n",
    "dataset = load_dataset(\"msaad02/brockport-gpt-4-qa\")['train'].to_pandas()\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context = torch.tensor([self.vocab[token] for token in self.tokenizer(context)])\n",
    "        target = torch.tensor([self.vocab[token] for token in self.tokenizer(target)])\n",
    "        return context, target\n",
    "\n",
    "def build_vocab(data, tokenizer):\n",
    "    token_generator = (token for _, sent in data for token in tokenizer(sent))\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        iterator = [token_generator], \n",
    "        specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"], \n",
    "        special_first=True, \n",
    "        min_freq=5\n",
    "    )\n",
    "    return vocab\n",
    "\n",
    "# def collate_fn(data: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "#     tensors, targets = zip(*data)\n",
    "#     features = pad_sequence(tensors, batch_first=True)\n",
    "#     targets = torch.stack(targets)\n",
    "#     return features, targets\n",
    "\n",
    "def collate_batch(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "    pad_idx = vocab['<pad>']\n",
    "    contexts = pad_sequence(contexts, padding_value=pad_idx, batch_first=True)\n",
    "    targets = pad_sequence(targets, padding_value=pad_idx, batch_first=True)\n",
    "    return contexts, targets\n",
    "\n",
    "# Create a list of tuples (context, target)\n",
    "data = list(zip(dataset['question'].tolist(), dataset['answer'].tolist()))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Randomly split data into train and validation\n",
    "split_idx = int(0.85 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# Define tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "vocab = build_vocab(train_data + val_data, tokenizer)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Seq2SeqDataset(train_data, vocab, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_data, vocab, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "# -------- Model ------------------------------------------------\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, dim_feedforward, dropout_rate, activation, vocab_length, batch_first=True):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "\n",
    "        self.context_embedding = nn.Embedding(vocab_length, d_model)\n",
    "        self.target_embedding = nn.Embedding(vocab_length, d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_rate,\n",
    "            activation=activation,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.context_embedding(src)\n",
    "        tgt = self.target_embedding(tgt)\n",
    "\n",
    "        return self.transformer(src, tgt)\n",
    "    \n",
    "\n",
    "model = Transformer(\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dim_feedforward=DFF,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    activation='relu',\n",
    "    vocab_length=len(vocab)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# -------- Training ------------------------------------------------\n",
    "pad_idx = vocab['<pad>']\n",
    "sos_idx = vocab['<sos>']\n",
    "eos_idx = vocab['<eos>']\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the training function\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for contexts, targets in train_loader:\n",
    "\n",
    "        # Move tensors to the right device\n",
    "        contexts = contexts.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        print(\"contexts\", contexts)\n",
    "        print(\"targets\", targets[:, :-1])\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(contexts, targets[:, :-1]) # Shifted targets for computing loss\n",
    "\n",
    "        # # Compute the loss\n",
    "        loss = loss_fn(output, targets[:, 1:])\n",
    "        # loss = loss_fn(output.reshape(-1, output.size(-1)), targets[1:, :].reshape(-1)) # Shifted targets for computing loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 111, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 23\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m contexts \u001b[38;5;241m=\u001b[39m contexts\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), targets[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/transformer.py:204\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[1;32m    207\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[1;32m    208\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    209\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    210\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    211\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the feature number of src and tgt must be equal to d_model"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for contexts, targets in data_loader:\n",
    "        contexts = contexts.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        output = model(contexts, targets[:, :-1])\n",
    "\n",
    "        \n",
    "        loss = loss_fn(output.reshape(-1, output.size(-1)), targets[:, 1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context = torch.tensor([self.vocab[token] for token in self.tokenizer(context)])\n",
    "        target = torch.tensor([self.vocab[token] for token in self.tokenizer(target)])\n",
    "        return context, target\n",
    "\n",
    "def collate_batch(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "    pad_idx = vocab['<pad>']\n",
    "    sos_idx = vocab['<sos>']\n",
    "    eos_idx = vocab['<eos>']\n",
    "\n",
    "    contexts = [torch.cat([torch.tensor([sos_idx]), context, torch.tensor([eos_idx])], dim=0) for context in contexts]\n",
    "    targets = [torch.cat([torch.tensor([sos_idx]), target, torch.tensor([eos_idx])], dim=0) for target in targets]\n",
    "\n",
    "    context_lens = torch.tensor([len(context) for context in contexts])\n",
    "    target_lens = torch.tensor([len(target) for target in targets])\n",
    "\n",
    "    contexts = pad_sequence(contexts, padding_value=pad_idx)\n",
    "    targets = pad_sequence(targets, padding_value=pad_idx)\n",
    "\n",
    "    return contexts, targets, context_lens, target_lens\n",
    "\n",
    "\n",
    "def build_vocab(data, tokenizer):\n",
    "    token_generator = (token for _, sent in data for token in tokenizer(sent))\n",
    "    return build_vocab_from_iterator(token_generator, specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"], special_first=True)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"msaad02/brockport-gpt-4-qa\")['train'].to_pandas()\n",
    "\n",
    "# Create a list of tuples (context, target)\n",
    "data = list(zip(dataset['question'].tolist(), dataset['answer'].tolist()))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Randomly split data into train and validation\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# Define tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab(train_data + val_data, tokenizer)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Seq2SeqDataset(train_data, vocab, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_data, vocab, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'can', 'i', 'apply', '?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"How can I apply?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 13\n",
    "BATCH_SIZE = 64\n",
    "NUM_LAYERS = 6\n",
    "D_MODEL = 512\n",
    "DFF = 2048\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "model = nn.Transformer(\n",
    "    d_model=D_MODEL,\n",
    "    nhead=NUM_HEADS,\n",
    "    num_encoder_layers=NUM_LAYERS,\n",
    "    num_decoder_layers=NUM_LAYERS,\n",
    "    dim_feedforward=DFF,\n",
    "    dropout=DROPOUT_RATE,\n",
    "    activation='relu',\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Token what not found and default index is not set\nException raised from __getitem__ at /__w/text/text/pytorch/text/torchtext/csrc/vocab.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6cc5d7dd87 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f6cc5d2e75f in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #2: torchtext::Vocab::__getitem__(c10::basic_string_view<char> const&) const + 0x384 (0x7f6bb22418b4 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so)\nframe #3: <unknown function> + 0x1dc5b (0x7f6c082a9c5b in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torchtext/_torchtext.so)\nframe #4: <unknown function> + 0x3dee7 (0x7f6c082c9ee7 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torchtext/_torchtext.so)\nframe #5: /home/msaad/miniconda3/envs/thesis/bin/python() [0x525d17]\nframe #6: _PyObject_MakeTpCall + 0x254 (0x502a14 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #7: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5541c3]\nframe #8: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b51d]\nframe #9: _PyEval_EvalFrameDefault + 0xfbd (0x50f5ed in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #10: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b0d0]\nframe #11: _PyEval_EvalFrameDefault + 0xfbd (0x50f5ed in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #12: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b0d0]\nframe #13: _PyEval_EvalFrameDefault + 0xfbd (0x50f5ed in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #14: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b75e]\nframe #15: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5a6cea]\nframe #16: _PyEval_EvalFrameDefault + 0x53b (0x50eb6b in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #17: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5c83fe]\nframe #18: PyEval_EvalCode + 0x9f (0x5c7aff in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #19: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5e1683]\nframe #20: _PyEval_EvalFrameDefault + 0x3a22 (0x512052 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #21: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #22: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #23: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #24: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #25: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #26: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5df096]\nframe #27: _PyEval_EvalFrameDefault + 0x35e4 (0x511c14 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #28: /home/msaad/miniconda3/envs/thesis/bin/python() [0x55451f]\nframe #29: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5541c3]\nframe #30: PyObject_Call + 0x9d (0x53ef0d in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #31: _PyEval_EvalFrameDefault + 0x42da (0x51290a in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #32: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #33: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #34: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #35: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #36: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #37: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #38: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #39: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #40: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #41: <unknown function> + 0x79e7 (0x7f6d5c0bb9e7 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so)\nframe #42: /home/msaad/miniconda3/envs/thesis/bin/python() [0x52405b]\nframe #43: /home/msaad/miniconda3/envs/thesis/bin/python() [0x4bf4de]\nframe #44: /home/msaad/miniconda3/envs/thesis/bin/python() [0x4c13c9]\nframe #45: /home/msaad/miniconda3/envs/thesis/bin/python() [0x51bef7]\nframe #46: _PyEval_EvalFrameDefault + 0x9353 (0x517983 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #47: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5c83fe]\nframe #48: PyEval_EvalCode + 0x9f (0x5c7aff in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #49: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5e1683]\nframe #50: /home/msaad/miniconda3/envs/thesis/bin/python() [0x51bef7]\nframe #51: PyObject_Vectorcall + 0x31 (0x51bde1 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #52: _PyEval_EvalFrameDefault + 0x753 (0x50ed83 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #53: _PyFunction_Vectorcall + 0x173 (0x534f13 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #54: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5f33ef]\nframe #55: Py_RunMain + 0x14a (0x5f2dfa in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #56: Py_BytesMain + 0x39 (0x5b6f49 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #57: <unknown function> + 0x29d90 (0x7f6d5d33fd90 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #58: __libc_start_main + 0x80 (0x7f6d5d33fe40 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #59: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5b6d9f]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Now call the train function\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     17\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src, tgt \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(src, tgt)\n\u001b[1;32m     20\u001b[0m         src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36mSeq2SeqDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     19\u001b[0m     context, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m---> 20\u001b[0m     context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     21\u001b[0m     target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(target)])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context, target\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     19\u001b[0m     context, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m---> 20\u001b[0m     context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(context)])\n\u001b[1;32m     21\u001b[0m     target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(target)])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context, target\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torchtext/vocab/vocab.py:65\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        token: The token used to lookup the corresponding index.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        The index corresponding to the associated token.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Token what not found and default index is not set\nException raised from __getitem__ at /__w/text/text/pytorch/text/torchtext/csrc/vocab.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6cc5d7dd87 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f6cc5d2e75f in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #2: torchtext::Vocab::__getitem__(c10::basic_string_view<char> const&) const + 0x384 (0x7f6bb22418b4 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so)\nframe #3: <unknown function> + 0x1dc5b (0x7f6c082a9c5b in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torchtext/_torchtext.so)\nframe #4: <unknown function> + 0x3dee7 (0x7f6c082c9ee7 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torchtext/_torchtext.so)\nframe #5: /home/msaad/miniconda3/envs/thesis/bin/python() [0x525d17]\nframe #6: _PyObject_MakeTpCall + 0x254 (0x502a14 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #7: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5541c3]\nframe #8: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b51d]\nframe #9: _PyEval_EvalFrameDefault + 0xfbd (0x50f5ed in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #10: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b0d0]\nframe #11: _PyEval_EvalFrameDefault + 0xfbd (0x50f5ed in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #12: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b0d0]\nframe #13: _PyEval_EvalFrameDefault + 0xfbd (0x50f5ed in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #14: /home/msaad/miniconda3/envs/thesis/bin/python() [0x57b75e]\nframe #15: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5a6cea]\nframe #16: _PyEval_EvalFrameDefault + 0x53b (0x50eb6b in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #17: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5c83fe]\nframe #18: PyEval_EvalCode + 0x9f (0x5c7aff in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #19: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5e1683]\nframe #20: _PyEval_EvalFrameDefault + 0x3a22 (0x512052 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #21: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #22: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #23: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #24: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #25: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #26: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5df096]\nframe #27: _PyEval_EvalFrameDefault + 0x35e4 (0x511c14 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #28: /home/msaad/miniconda3/envs/thesis/bin/python() [0x55451f]\nframe #29: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5541c3]\nframe #30: PyObject_Call + 0x9d (0x53ef0d in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #31: _PyEval_EvalFrameDefault + 0x42da (0x51290a in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #32: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #33: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #34: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #35: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #36: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #37: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #38: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #39: _PyEval_EvalFrameDefault + 0x32d9 (0x511909 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #40: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5dc8ca]\nframe #41: <unknown function> + 0x79e7 (0x7f6d5c0bb9e7 in /home/msaad/miniconda3/envs/thesis/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so)\nframe #42: /home/msaad/miniconda3/envs/thesis/bin/python() [0x52405b]\nframe #43: /home/msaad/miniconda3/envs/thesis/bin/python() [0x4bf4de]\nframe #44: /home/msaad/miniconda3/envs/thesis/bin/python() [0x4c13c9]\nframe #45: /home/msaad/miniconda3/envs/thesis/bin/python() [0x51bef7]\nframe #46: _PyEval_EvalFrameDefault + 0x9353 (0x517983 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #47: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5c83fe]\nframe #48: PyEval_EvalCode + 0x9f (0x5c7aff in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #49: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5e1683]\nframe #50: /home/msaad/miniconda3/envs/thesis/bin/python() [0x51bef7]\nframe #51: PyObject_Vectorcall + 0x31 (0x51bde1 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #52: _PyEval_EvalFrameDefault + 0x753 (0x50ed83 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #53: _PyFunction_Vectorcall + 0x173 (0x534f13 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #54: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5f33ef]\nframe #55: Py_RunMain + 0x14a (0x5f2dfa in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #56: Py_BytesMain + 0x39 (0x5b6f49 in /home/msaad/miniconda3/envs/thesis/bin/python)\nframe #57: <unknown function> + 0x29d90 (0x7f6d5d33fd90 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #58: __libc_start_main + 0x80 (0x7f6d5d33fe40 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #59: /home/msaad/miniconda3/envs/thesis/bin/python() [0x5b6d9f]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define a loss function, ignoring the padding index in the loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def train(model, data_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for src, tgt in data_loader:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            tgt_input = tgt[:-1, :]\n",
    "            targets = tgt[1:, :]  # targets do not include the <sos> token\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, pad_idx=vocab['<pad>'])\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "\n",
    "            loss = criterion(output, targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "def create_masks(src, tgt, pad_idx):\n",
    "    src_seq_len = src.size(0)\n",
    "    tgt_seq_len = tgt.size(0)\n",
    "\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
    "    \n",
    "    src_padding_mask = (src == pad_idx).transpose(0, 1).to(device)\n",
    "    tgt_padding_mask = (tgt == pad_idx).transpose(0, 1).to(device)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# Now call the train function\n",
    "train(model, train_loader, optimizer, criterion, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"msaad02/brockport-gpt-4-qa\")['train'].to_pandas()\n",
    "\n",
    "# Create a list of tuples (context, target)\n",
    "data = list(zip(dataset['question'].tolist(), dataset['answer'].tolist()))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Randomly split data into train and validation\n",
    "split_idx = int(0.85 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# Define tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "\n",
    "def build_vocab(data, tokenizer):\n",
    "    token_generator = (token for _, sent in data for token in tokenizer(sent))\n",
    "    return build_vocab_from_iterator([token_generator], specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"], special_first=True, min_freq=5)\n",
    "\n",
    "\n",
    "vocab = build_vocab(train_data + val_data, tokenizer)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context = torch.tensor([self.vocab[token] for token in self.tokenizer(context)])\n",
    "        target = torch.tensor([self.vocab[token] for token in self.tokenizer(target)])\n",
    "        return context, target\n",
    "    \n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Seq2SeqDataset(train_data, vocab, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_data, vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_batch(batch):\n",
    "#     contexts, targets = zip(*batch)\n",
    "#     pad_idx = vocab['<pad>']\n",
    "#     sos_idx = vocab['<sos>']\n",
    "#     eos_idx = vocab['<eos>']\n",
    "\n",
    "#     contexts = [torch.cat([torch.tensor([sos_idx]), context, torch.tensor([eos_idx])], dim=0) for context in contexts]\n",
    "#     targets = [torch.cat([torch.tensor([sos_idx]), target, torch.tensor([eos_idx])], dim=0) for target in targets]\n",
    "\n",
    "#     context_lens = torch.tensor([len(context) for context in contexts])\n",
    "#     target_lens = torch.tensor([len(target) for target in targets])\n",
    "\n",
    "#     contexts = pad_sequence(contexts, padding_value=pad_idx)\n",
    "#     targets = pad_sequence(targets, padding_value=pad_idx)\n",
    "\n",
    "#     return contexts, targets, context_lens, target_lens\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "i = 1\n",
    "def collate_fn(data: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    global i\n",
    "    i += 1\n",
    "    tensors, targets = zip(*data)\n",
    "    features = pad_sequence(tensors, batch_first=True)\n",
    "    targets = torch.stack(targets)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f8b6026b940>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensors, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m pad_sequence(tensors, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(targets)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "tensors, targets = zip(*data)\n",
    "features = pad_sequence(tensors, batch_first=True)\n",
    "targets = torch.stack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 212,   20,  767,   95,   42,   19,  437,  287,    6,   69,  767,  359,\n",
       "          491,  504, 1766]),\n",
       " tensor([ 35,   9,  44, 137,  19, 437, 287,   4,   7,  46, 160, 135,  18,  11,\n",
       "         121, 310,   5,   9,  20,  80,  74,  48,   6,  12,  46, 258,  13, 380,\n",
       "         195,   5,  27,  10,  29,  89,   6,  42,   9, 248,  26, 303,  19,  12,\n",
       "          46, 218,  16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Assuming 'vocab' is your vocabulary object from torchtext\n",
    "vocab_path = \"vocab.pkl\"\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    loaded_vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where can i get help with class registration to ensure i maintain full-time status ?\n",
      "if you need assistance with class registration , the academic success center is a fantastic resource . you can also reach out to your academic advisor for personalized guidance . we ' re here to help you stay on track with your academic goals !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\" \".join(vocab.lookup_tokens(train_dataset[0][0].cpu().numpy().tolist())))\n",
    "print(\" \".join(vocab.lookup_tokens(train_dataset[0][1].cpu().numpy().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where can i get help with class registration to ensure i maintain full-time status ?\n",
      "if you need assistance with class registration , the academic success center is a fantastic resource . you can also reach out to your academic advisor for personalized guidance . we ' re here to help you stay on track with your academic goals !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\" \".join(loaded_vocab.lookup_tokens(train_dataset[0][0].cpu().numpy().tolist())))\n",
    "print(\" \".join(loaded_vocab.lookup_tokens(train_dataset[0][1].cpu().numpy().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1816"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 325,   30,    7,  541,  158,   13,  313,   32,  217,   15,    7,  610,\n",
       "            8, 2278,  122,   40, 1766]),\n",
       " tensor([  27,   10,   29,  174,    9,   10,   29,  475, 1575,    6,   25,   21,\n",
       "           16,   13,  313,   32,  264,    6,  260,    7,  610,    8, 2278,  122,\n",
       "           40,    4,    7,  402,  355,   13,    7,  288,  115,   18,  702,  343,\n",
       "            5,   63,  111,    6,   95,   12,   77,   15,   36,  382,    6,  194,\n",
       "           12,  196,   19,  144,   16]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142, 20, 767, 99, 1766]\n",
      "['how', 'can', 'i', 'apply', '?']\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I apply?\"\n",
    "inputs = tokenizer(question)\n",
    "\n",
    "ids = vocab.forward(inputs)\n",
    "print(ids)\n",
    "\n",
    "txt = vocab.lookup_tokens(ids)\n",
    "print(txt)\n",
    "\n",
    "# vocab.get_itos()[5896]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = vocab.lookup_tokens([0])\n",
    "txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
