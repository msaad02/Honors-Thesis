{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.3367\n",
      "Epoch 2, Loss: 5.0624\n",
      "Epoch 3, Loss: 4.4954\n",
      "Epoch 4, Loss: 4.1974\n",
      "Epoch 5, Loss: 3.9810\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For some reason, most likely due to how the model is implemented, the ScratchModel\n",
    "runs EXCEPTIONALLY slow. Again, I'm not sure why exactly, but the scratch model is\n",
    "a small transformer model that should not be running this slow. This script aims to\n",
    "re-implement the  ScratchModel in PyTorch using their optimized Transformer module\n",
    "and see if that fixes the issue.\n",
    "\n",
    "All the parameters will be the same, so we can just copy them over. The only thing\n",
    "that will change is the model implementation and data loading.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # prevent tensorflow logs\n",
    "\n",
    "# # Set path to parent directory so we can import from other folders.\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from typing import List, Tuple\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------- Model ------------------------------------------------\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "NUM_LAYERS = 6\n",
    "D_MODEL = 512\n",
    "DFF = 2048\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "\n",
    "# -------- Data ------------------------------------------------\n",
    "\n",
    "dataset = load_dataset(\"msaad02/brockport-gpt-4-qa\")['train'].to_pandas()\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.sos_idx = vocab['[START]']\n",
    "        self.eos_idx = vocab['[END]']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context = torch.tensor([self.vocab[token] for token in self.tokenizer(context)])\n",
    "        target = torch.tensor([self.vocab[token] for token in self.tokenizer(target)])\n",
    "        target = torch.cat([torch.tensor([self.sos_idx]), target, torch.tensor([self.eos_idx])])\n",
    "        return context, target\n",
    "\n",
    "def build_vocab(data, tokenizer):\n",
    "    token_generator = (token for _, sent in data for token in tokenizer(sent))\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        iterator = [token_generator], \n",
    "        specials=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\"],\n",
    "        special_first=True, \n",
    "        min_freq=5\n",
    "    )\n",
    "    return vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "    pad_idx = vocab['[PAD]']\n",
    "    contexts = pad_sequence(contexts, padding_value=pad_idx, batch_first=True)\n",
    "    targets = pad_sequence(targets, padding_value=pad_idx, batch_first=True)\n",
    "    return contexts, targets\n",
    "\n",
    "# Create a list of tuples (context, target)\n",
    "context = dataset['question'].tolist()\n",
    "target = dataset['answer'].tolist()\n",
    "\n",
    "data = list(zip(context, target))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Randomly split data into train and validation\n",
    "split_idx = int(0.85 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# Define tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "vocab = build_vocab(train_data + val_data, tokenizer)\n",
    "vocab.set_default_index(vocab[\"[UNK]\"])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Seq2SeqDataset(train_data, vocab, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_data, vocab, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "# -------- Model ------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dtype, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x).type(self.dtype)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, dim_feedforward, dropout_rate, activation, vocab_length, dtype, batch_first=True):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.context_embedding = nn.Embedding(vocab_length, d_model, dtype=dtype)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dtype=dtype, dropout=dropout_rate, max_len=256)\n",
    "\n",
    "        self.target_embedding = nn.Embedding(vocab_length, d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_rate,\n",
    "            activation=activation,\n",
    "            batch_first=batch_first,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, vocab_length, dtype=dtype)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.context_embedding(src)\n",
    "        tgt = self.target_embedding(tgt)\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = Transformer(\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dim_feedforward=DFF,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    activation='relu',\n",
    "    vocab_length=len(vocab),\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# -------- Training ------------------------------------------------\n",
    "pad_idx = vocab['[PAD]']\n",
    "sos_idx = vocab['[START]']\n",
    "eos_idx = vocab['[END]']\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for contexts, targets in train_loader:\n",
    "        # Move tensors to the right device\n",
    "        src = contexts.to(device)\n",
    "        tgt = targets.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]  # All tokens except the last (remove <eos>)\n",
    "        tgt_output = tgt[:, 1:]  # All tokens except the first (remove <sos>)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(src, tgt_input)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(out.reshape(-1, out.shape[-1]), tgt_output.reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "def predict_sentence(model: Transformer, sentence: str, vocab: torchtext.vocab.Vocab, tokenizer, max_len: int = 50):\n",
    "    model.eval()\n",
    "\n",
    "    src = tokenizer(sentence)\n",
    "    src = vocab.lookup_indices(src)\n",
    "    src = torch.tensor(src).unsqueeze(0).to(device)\n",
    "\n",
    "    pad_idx = vocab['[PAD]']\n",
    "    start_idx = vocab['[START]']\n",
    "\n",
    "    out = torch.zeros((1, max_len+1)).type_as(src.data)\n",
    "    out.fill_(pad_idx)\n",
    "    out[0, 0] = start_idx\n",
    "\n",
    "    ctx = model.context_embedding(src)\n",
    "    ctx = model.pos_encoder(ctx)\n",
    "\n",
    "    next_symbol = start_idx\n",
    "\n",
    "    for i in range(0, max_len+1):\n",
    "        out[0][i] = next_symbol\n",
    "\n",
    "        tgt = model.target_embedding(out)\n",
    "        tgt = model.pos_encoder(tgt)\n",
    "\n",
    "        tgt = model.transformer(ctx, tgt)\n",
    "        tgt = model.classifier(tgt)\n",
    "\n",
    "        next_symbol = torch.argmax(tgt[0,0]).item()\n",
    "\n",
    "    return \" \".join(vocab.lookup_tokens(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\n",
    "    model=model,\n",
    "    sentence=\"How can I apply to SUNY Brockport?\",\n",
    "    vocab=vocab,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when are new [UNK] of the port released ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "how does the selection process work for the fast program and what happens if i ' m selected ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "what support does suny brockport offer to english students facing financial emergencies ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "how can i get in touch with professor [UNK] [UNK] for academic advising ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "what are the benefits of attending scholars day ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "what qualifications do i need to become a peer mentor ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "what kind of support do gold leadership advisors provide to their student groups ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "what steps should i take after [UNK] an online cad or save workshop ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "what measures does suny brockport take to ensure customer satisfaction with its facilities ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n",
      "what kind of hands-on experiences can i expect in the earth science adolescence education program at suny brockport ?\n",
      "[START] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    i += 50\n",
    "\n",
    "    sentence = \" \".join(vocab.lookup_tokens(train_dataset.__getitem__(i)[0].tolist()))\n",
    "\n",
    "    response = predict_sentence(\n",
    "        model=model,\n",
    "        sentence=sentence,\n",
    "        vocab=vocab,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=50\n",
    "    )\n",
    "\n",
    "    print(sentence)\n",
    "    print(response, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Transformer(\n",
       "  (context_embedding): Embedding(5898, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (target_embedding): Embedding(5898, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=5898, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53205770"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
