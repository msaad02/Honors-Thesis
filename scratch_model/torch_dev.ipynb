{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msaad/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/transformer.py:384: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.4581, Val Loss: 5.8376\n",
      "Epoch 2, Loss: 5.7990, Val Loss: 6.1304\n",
      "Epoch 3, Loss: 6.1353, Val Loss: 6.1445\n",
      "Epoch 4, Loss: 6.1337, Val Loss: 6.1327\n",
      "Epoch 5, Loss: 6.1238, Val Loss: 6.1249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 311\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 311\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, loss_fn, device)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 275\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m    273\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    274\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 275\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n",
      "Cell \u001b[0;32mIn[1], line 131\u001b[0m, in \u001b[0;36mNoamOptim.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    130\u001b[0m     p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    372\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 373\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/autograd/profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 622\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/_ops.py:513\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For some reason, most likely due to how the model is implemented, the ScratchModel\n",
    "runs EXCEPTIONALLY slow. Again, I'm not sure why exactly, but the scratch model is\n",
    "a small transformer model that should not be running this slow. This script aims to\n",
    "re-implement the  ScratchModel in PyTorch using their optimized Transformer module\n",
    "and see if that fixes the issue.\n",
    "\n",
    "All the parameters will be the same, so we can just copy them over. The only thing\n",
    "that will change is the model implementation and data loading.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # prevent tensorflow logs\n",
    "\n",
    "# # Set path to parent directory so we can import from other folders.\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from typing import List, Tuple\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------- Model ------------------------------------------------\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "NUM_LAYERS = 6\n",
    "D_MODEL = 512\n",
    "DFF = 2048\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "\n",
    "# -------- Data ------------------------------------------------\n",
    "\n",
    "dataset = load_dataset(\"msaad02/brockport-gpt-4-qa\")['train'].to_pandas()\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.sos_idx = vocab['[START]']\n",
    "        self.eos_idx = vocab['[END]']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context = torch.tensor([self.vocab[token] for token in self.tokenizer(context)])\n",
    "        target = torch.tensor([self.vocab[token] for token in self.tokenizer(target)])\n",
    "        target = torch.cat([torch.tensor([self.sos_idx]), target, torch.tensor([self.eos_idx])])\n",
    "        return context, target\n",
    "\n",
    "def build_vocab(data, tokenizer):\n",
    "    token_generator = (token for _, sent in data for token in tokenizer(sent))\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        iterator = [token_generator], \n",
    "        specials=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\"],\n",
    "        special_first=True, \n",
    "        min_freq=5\n",
    "    )\n",
    "    return vocab\n",
    "\n",
    "def collate_batch(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "    pad_idx = vocab['[PAD]']\n",
    "    contexts = pad_sequence(contexts, padding_value=pad_idx, batch_first=True)\n",
    "    targets = pad_sequence(targets, padding_value=pad_idx, batch_first=True)\n",
    "    return contexts, targets\n",
    "\n",
    "# Create a list of tuples (context, target)\n",
    "context = dataset['question'].tolist()\n",
    "target = dataset['answer'].tolist()\n",
    "\n",
    "data = list(zip(context, target))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Randomly split data into train and validation\n",
    "split_idx = int(0.85 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# Define tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "vocab = build_vocab(train_data + val_data, tokenizer)\n",
    "vocab.set_default_index(vocab[\"[UNK]\"])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Seq2SeqDataset(train_data, vocab, tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_data, vocab, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, drop_last=True)\n",
    "\n",
    "\n",
    "# -------- Model ------------------------------------------------\n",
    "class NoamOptim(object):\n",
    "    \"Optimizer wrapper for learning rate scheduling.\"\n",
    "    # https://colab.research.google.com/github/jaygala24/pytorch-implementations/blob/master/Attention%20Is%20All%20You%20Need.ipynb#scrollTo=pmwvcO8zpNeT\n",
    "    def __init__(self, optimizer, d_model, factor, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.factor = factor\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.n_steps += 1\n",
    "        lr = self.get_lr()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.factor * (\n",
    "            self.d_model ** (-0.5)\n",
    "            * min(self.n_steps ** (-0.5), self.n_steps * self.n_warmup_steps ** (-1.5))\n",
    "        )\n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dtype, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x).type(self.dtype)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, dim_feedforward, dropout_rate, activation, vocab_length, dtype, batch_first=True):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.context_embedding = nn.Embedding(vocab_length, d_model, dtype=dtype)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dtype=dtype, dropout=dropout_rate, max_len=256)\n",
    "\n",
    "        self.target_embedding = nn.Embedding(vocab_length, d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_rate,\n",
    "            activation=activation,\n",
    "            batch_first=batch_first,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, vocab_length, dtype=dtype)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, src_key_padding_mask = None, tgt_key_padding_mask = None):\n",
    "        src = self.context_embedding(src)\n",
    "        tgt = self.target_embedding(tgt)\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        if src_key_padding_mask is None and tgt_key_padding_mask is None:\n",
    "            # For inference on single examples\n",
    "            out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        else:\n",
    "            # For training (or inference if batching)\n",
    "            out = self.transformer(\n",
    "                src, tgt,\n",
    "                tgt_mask=tgt_mask,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=src_key_padding_mask                   \n",
    "            )\n",
    "\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = Transformer(\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dim_feedforward=DFF,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    activation='relu',\n",
    "    vocab_length=len(vocab),\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# -------- Masking ------------------------------------------------\n",
    "# Helper function to create a mask of size 'sz'\n",
    "def generate_square_subsequent_mask(sz: int):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 0).transpose(0, 1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "# -------- Training ------------------------------------------------\n",
    "pad_idx = vocab['[PAD]']\n",
    "sos_idx = vocab['[START]']\n",
    "eos_idx = vocab['[END]']\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = CrossEntropyLoss(ignore_index=pad_idx)#, label_smoothing=0.1)\n",
    "# optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "optimizer = NoamOptim(\n",
    "    optimizer=Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9),\n",
    "    d_model=D_MODEL,\n",
    "    factor=1,\n",
    "    n_warmup_steps=500\n",
    ")\n",
    "\n",
    "# Define the training function\n",
    "def train(model, data_loader, optimizer, loss_fn, device):\n",
    "    global out, tgt_output, src, tgt, tgt_input\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for contexts, targets in train_loader:\n",
    "        # Move tensors to the right device\n",
    "        src = contexts.to(device)\n",
    "        tgt = targets.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]  # All tokens except the last (remove <eos>)\n",
    "        tgt_output = tgt[:, 1:]  # All tokens except the first (remove <sos>)\n",
    "\n",
    "        # Generate masks\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "        src_key_padding_mask = (src == pad_idx)\n",
    "        tgt_key_padding_mask = (tgt_input == pad_idx)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(src, tgt_input, tgt_mask, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(out.transpose(1,2), tgt_output) # make out in shape (N, L, C) and tgt_output in shape (N, L)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in data_loader:\n",
    "            src = contexts.to(device)\n",
    "            tgt = targets.to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]  # All tokens except the last (remove <eos>)\n",
    "            tgt_output = tgt[:, 1:]  # All tokens except the first (remove <sos>)\n",
    "\n",
    "            # Generate masks\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "            src_key_padding_mask = (src == pad_idx)\n",
    "            tgt_key_padding_mask = (tgt_input == pad_idx)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(src, tgt_input, tgt_mask, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(out.transpose(1,2), tgt_output)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    # print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_sentence(model, sentence, vocab, tokenizer, max_len=50, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    src = tokenizer(sentence)\n",
    "    src_indices = vocab.lookup_indices(src)\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    pad_idx = vocab['[PAD]']\n",
    "    start_idx = vocab['[START]']\n",
    "\n",
    "    out = torch.full((1, max_len), fill_value=pad_idx, dtype=torch.long).to(device)\n",
    "    out[:, 0] = start_idx\n",
    "\n",
    "    for i in range(1, max_len):  # Start at 1 since we already placed the [START] token\n",
    "        tgt_mask = generate_square_subsequent_mask(i+1).to(device)  # Mask for current target length\n",
    "        output = model(src_tensor, out[:, :i+1], tgt_mask=tgt_mask)\n",
    "        next_symbol = output[:, -1:].argmax(-1)  # Predict the next token based on the last position\n",
    "        out[:, i] = next_symbol\n",
    "\n",
    "    result_tokens = vocab.lookup_tokens(out[0].tolist())\n",
    "    # Filter out any tokens after an \"[END]\" token\n",
    "    end_idx = result_tokens.index('[END]') if '[END]' in result_tokens else len(result_tokens)\n",
    "    return \" \".join(result_tokens[:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\n",
    "    model=model,\n",
    "    sentence=\"How can I apply to SUNY Brockport?\",\n",
    "    vocab=vocab,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where can i purchase a parking permit for suny brockport ' s performance venues ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "when should i see a healthcare provider for a sore throat ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "how can i get in touch with the suny brockport ' s registrar ' s office ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "i completed my graduate degree at another institution , how can i apply for professional certification ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "what are the goals of brockport summer learning , and how do they align with the rochester model for high-quality summer learning ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "what kind of support can i expect from faculty advisors at suny brockport while pursuing a bachelor of science in mathematics with adolescence education ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "what is the contact information for the associate dean of the school of education , health , and human services ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "will my gpa from all previous institutions affect my eligibility for a transfer scholarship ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "can i view the summersession course schedule before registration opens ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n",
      "how soon after applying to the internship program can i expect to be admitted ?\n",
      "[START] is for , , , is , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    i += 50\n",
    "\n",
    "    sentence = \" \".join(vocab.lookup_tokens(train_dataset.__getitem__(i)[0].tolist()))\n",
    "\n",
    "    response = predict_sentence(\n",
    "        model=model,\n",
    "        sentence=sentence,\n",
    "        vocab=vocab,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=50\n",
    "    )\n",
    "\n",
    "    print(sentence)\n",
    "    print(response, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Transformer(\n",
       "  (context_embedding): Embedding(5898, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (target_embedding): Embedding(5898, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=5898, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53205770"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
