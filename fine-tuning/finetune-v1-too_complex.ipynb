{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "from peft.utils.other import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING as model_to_lora_modules\n",
    "\n",
    "# Set up your arbitrary parameters here\n",
    "lora_name = \"example_model\"\n",
    "always_override = False\n",
    "save_steps = 2000\n",
    "micro_batch_size = 4\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "learning_rate = \"3e-4\"\n",
    "lr_scheduler_type = \"linear\"\n",
    "lora_rank = 32\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.05\n",
    "cutoff_len = 256\n",
    "dataset = \"example_dataset\"\n",
    "eval_dataset = None\n",
    "format = \"example_format\"\n",
    "eval_steps = 1000\n",
    "raw_text_file = None\n",
    "overlap_len = 128\n",
    "newline_favor_len = 128\n",
    "higher_rank_limit = False\n",
    "warmup_steps = 100\n",
    "optimizer = \"adamw_torch\"\n",
    "hard_cut_string = \"\\n\\n\\n\"\n",
    "train_only_after = \"\"\n",
    "stop_at_loss = 0.0\n",
    "add_eos_token = False\n",
    "min_chars = 0\n",
    "report_to = \"None\"\n",
    "\n",
    "# Other global variables\n",
    "PARAMETERS = [\"lora_name\", \"always_override\", \"save_steps\", \"micro_batch_size\", \"batch_size\", \"epochs\", \"learning_rate\",\n",
    "              \"lr_scheduler_type\", \"lora_rank\", \"lora_alpha\", \"lora_dropout\", \"cutoff_len\", \"dataset\", \"eval_dataset\",\n",
    "              \"format\", \"eval_steps\", \"raw_text_file\", \"overlap_len\", \"newline_favor_len\", \"higher_rank_limit\",\n",
    "              \"warmup_steps\", \"optimizer\", \"hard_cut_string\", \"train_only_after\", \"stop_at_loss\", \"add_eos_token\",\n",
    "              \"min_chars\", \"report_to\"]\n",
    "WANT_INTERRUPT = False\n",
    "\n",
    "def clean_path(base_path: str, path: str):\n",
    "    path = path.replace(\"\\\\\", \"/\").replace(\"..\", \"_\")\n",
    "    if base_path is None:\n",
    "        return path\n",
    "    return f\"{Path(base_path).absolute()}/{path}\"\n",
    "\n",
    "def backup_adapter(input_folder):\n",
    "    # Get the creation date of the file adapter_model.bin\n",
    "    try:\n",
    "        adapter_file = Path(f\"{input_folder}/adapter_model.bin\")\n",
    "        if adapter_file.is_file():\n",
    "            print(\"Backing up existing LoRA adapter...\")\n",
    "            creation_date = datetime.fromtimestamp(adapter_file.stat().st_ctime)\n",
    "            creation_date_str = creation_date.strftime(\"Backup-%Y-%m-%d\")\n",
    "\n",
    "            # Create the new subfolder\n",
    "            subfolder_path = Path(f\"{input_folder}/{creation_date_str}\")\n",
    "            subfolder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Check if the file already exists in the subfolder\n",
    "            backup_adapter_file = Path(f\"{input_folder}/{creation_date_str}/adapter_model.bin\")\n",
    "            if backup_adapter_file.is_file():\n",
    "                print(\" - Backup already exists. Skipping backup process.\")\n",
    "                return\n",
    "\n",
    "            # Copy existing files to the new subfolder\n",
    "            existing_files = Path(input_folder).iterdir()\n",
    "            for file in existing_files:\n",
    "                if file.is_file():\n",
    "                    shutil.copy2(file, subfolder_path)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in backup_adapter:\", str(e))\n",
    "\n",
    "def calc_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    return trainable_params, all_param\n",
    "\n",
    "def split_chunks(arr, size, step):\n",
    "    for i in range(0, len(arr), step):\n",
    "        yield arr[i:i + size]\n",
    "\n",
    "def cut_chunk_for_newline(chunk: str, max_length: int):\n",
    "    if '\\n' not in chunk:\n",
    "        return chunk\n",
    "    first_newline = chunk.index('\\n')\n",
    "    if first_newline < max_length:\n",
    "        chunk = chunk[first_newline + 1:]\n",
    "    if '\\n' not in chunk:\n",
    "        return chunk\n",
    "    last_newline = chunk.rindex('\\n')\n",
    "    if len(chunk) - last_newline < max_length:\n",
    "        chunk = chunk[:last_newline]\n",
    "    return chunk\n",
    "\n",
    "def tokenize(prompt, append_eos_token=False):\n",
    "    if train_only_after == '' or train_only_after not in prompt:\n",
    "        input_ids = encode(prompt, True)\n",
    "        if append_eos_token and input_ids[-1] != shared.tokenizer.eos_token_id and len(input_ids) < cutoff_len:\n",
    "            input_ids.append(shared.tokenizer.eos_token_id)\n",
    "        input_ids = [shared.tokenizer.pad_token_id] * (cutoff_len - len(input_ids)) + input_ids\n",
    "        labels = [1] * len(input_ids)\n",
    "    else:\n",
    "        ind = prompt.index(train_only_after) + len(train_only_after)\n",
    "        before_tokens = encode(prompt[:ind], True)\n",
    "        after_tokens = encode(prompt[ind:], False)\n",
    "        if append_eos_token and after_tokens[-1] != shared.tokenizer.eos_token_id:\n",
    "            after_tokens.append(shared.tokenizer.eos_token_id)\n",
    "        full_length = len(after_tokens) + len(before_tokens)\n",
    "        if full_length > cutoff_len:\n",
    "            after_tokens = after_tokens[:cutoff_len - len(before_tokens)]\n",
    "        else:\n",
    "            before_tokens = [shared.tokenizer.pad_token_id] * (cutoff_len - full_length) + before_tokens\n",
    "        input_ids = before_tokens + after_tokens\n",
    "        labels = [-100] * len(before_tokens) + [1] * len(after_tokens)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": input_ids.ne(shared.tokenizer.pad_token_id),\n",
    "    }\n",
    "\n",
    "def do_train():\n",
    "    if not hasattr(shared.model, 'lm_head') or hasattr(shared.model.lm_head, 'weight'):\n",
    "        prepare_model_for_int8_training(shared.model)\n",
    "\n",
    "    lora_file_path = clean_path(shared.args.lora_dir, lora_name)\n",
    "    actual_save_steps = math.ceil(save_steps / gradient_accumulation_steps)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=model_to_lora_modules[model_id],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    shared.model = 3 # transformer model from huggingface\n",
    "    \n",
    "    model_trainable_params, model_all_params = calc_trainable_parameters(shared.model)\n",
    "\n",
    "    try:\n",
    "        lora_model = get_peft_model(shared.model, config)\n",
    "\n",
    "        if not always_override and Path(f\"{lora_file_path}/adapter_model.bin\").is_file():\n",
    "            state_dict_peft = torch.load(f\"{lora_file_path}/adapter_model.bin\")\n",
    "            set_peft_model_state_dict(lora_model, state_dict_peft)\n",
    "\n",
    "    except:\n",
    "        return traceback.format_exc()\n",
    "\n",
    "    lora_model.config.use_cache = False\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        lora_model = torch.compile(lora_model)\n",
    "\n",
    "    train_log = {}\n",
    "    lora_trainable_param, lora_all_param = calc_trainable_parameters(lora_model)\n",
    "    configs = locals()\n",
    "\n",
    "    with open(f\"{lora_file_path}/training_parameters.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({x: configs[x] for x in PARAMETERS}, file, indent=2)\n",
    "\n",
    "    def threaded_run():\n",
    "        trainer = transformers.Trainer(\n",
    "            model=lora_model,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=eval_data,\n",
    "            args=transformers.TrainingArguments(\n",
    "                report_to=report_to if report_to != \"None\" else None,\n",
    "                per_device_train_batch_size=micro_batch_size,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                warmup_steps=math.ceil(warmup_steps / gradient_accumulation_steps),\n",
    "                num_train_epochs=epochs,\n",
    "                learning_rate=actual_lr,\n",
    "                fp16=False if shared.args.cpu else True,\n",
    "                optim=optimizer,\n",
    "                logging_steps=2 if stop_at_loss > 0 else 5,\n",
    "                evaluation_strategy=\"steps\" if eval_data is not None else \"no\",\n",
    "                eval_steps=math.ceil(eval_steps / gradient_accumulation_steps) if eval_data is not None else None,\n",
    "                save_strategy=\"steps\" if eval_data is not None else \"no\",\n",
    "                output_dir=lora_file_path,\n",
    "                lr_scheduler_type=lr_scheduler_type,\n",
    "                load_best_model_at_end=eval_data is not None,\n",
    "                ddp_find_unused_parameters=None,\n",
    "                no_cuda=shared.args.cpu,\n",
    "            ),\n",
    "            data_collator=transformers.DataCollatorForLanguageModeling(shared.tokenizer, mlm=False),\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "    thread = threading.Thread(target=threaded_run)\n",
    "    thread.start()\n",
    "    last_step = 0\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    while thread.is_alive():\n",
    "        time.sleep(0.5)\n",
    "        if WANT_INTERRUPT:\n",
    "            return \"Training interrupted\"\n",
    "\n",
    "        elif tracked.current_steps != last_step:\n",
    "            last_step = tracked.current_steps\n",
    "            time_elapsed = time.perf_counter() - start_time\n",
    "            if time_elapsed <= 0:\n",
    "                timer_info = \"\"\n",
    "                total_time_estimate = 999\n",
    "            else:\n",
    "                its = tracked.current_steps / time_elapsed\n",
    "                if its > 1:\n",
    "                    timer_info = f\"`{its:.2f}` it/s\"\n",
    "                else:\n",
    "                    timer_info = f\"`{1.0/its:.2f}` s/it\"\n",
    "\n",
    "                total_time_estimate = (1.0 / its) * (tracked.max_steps)\n",
    "\n",
    "            return f\"Running... {timer_info}, {format_time(time_elapsed)} / {format_time(total_time_estimate)} ... {format_time(total_time_estimate - time_elapsed)} remaining\"\n",
    "\n",
    "    if not tracked.did_save:\n",
    "        lora_model.save_pretrained(lora_file_path)\n",
    "\n",
    "    if WANT_INTERRUPT:\n",
    "        return \"Training interrupted\"\n",
    "\n",
    "    return \"Training complete!\"\n",
    "\n",
    "def format_time(seconds: float):\n",
    "    if seconds < 120:\n",
    "        return f\"`{seconds:.0f}` seconds\"\n",
    "    minutes = seconds / 60\n",
    "    if minutes < 120:\n",
    "        return f\"`{minutes:.0f}` minutes\"\n",
    "    hours = minutes / 60\n",
    "    return f\"`{hours:.0f}` hours\"\n",
    "\n",
    "result = do_train()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
