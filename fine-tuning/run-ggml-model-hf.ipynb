{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Its broken\n",
    "\n",
    "Not sure what is going on here. Possibly a failed quantization? Would like to investigate this further.\n",
    "\n",
    "Note that this happens on almost all loaders, EXCEPT from llama.cpp directly. It is possible to get a response from it by navigating to the llama.cpp folder and running like this: (output after)\n",
    "\n",
    "`./main -m /home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin -p \"Below is an inquiry related to SUNY Brockport - from academics, admissions, and faculty support to student life. Prioritize accuracy and brevity.\\n\\n### Instruction:\\nWhat are the steps to apply?\\n\\n### Response:\\n\"`\n",
    "\n",
    "![proof that ggml works](/home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/ggml-works-in-llamacpp.png)\n",
    "\n",
    "---\n",
    "This is incredibly strange behavior, especially since the llama.cpp works. Super strange that the python binding doesn't, since it's just build exactly on top of existing llama.cpp code.\n",
    "\n",
    "Things I've tried:\n",
    "- Rerunning quantization on different script (tried TheBloke script and one provided in llama.cpp examples)\n",
    "- Different quant methods (q4_0, q4_1, q8_0, etc.)\n",
    "- Different interfaces (python binding, ctransformers, text-generation-webui)\n",
    "\n",
    "Maybe try gguf? I think this is a brand BRAND new quant technique. Might be worth checking out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin\n",
      "error loading model: unknown (magic, version) combination: 46554747, 00000001; is this really a GGML file?\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[0;32m----> 3\u001b[0m llm \u001b[39m=\u001b[39m Llama(model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/llama_cpp/llama.py:328\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[39mwith\u001b[39;00m suppress_stdout_stderr():\n\u001b[1;32m    325\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m    326\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n\u001b[1;32m    327\u001b[0m         )\n\u001b[0;32m--> 328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_new_context_with_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"/home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading model: unknown (magic, version) combination: 46554747, 00000001; is this really a GGML file?\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create LLM 'llama' from '/home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m llm \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m/home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/ctransformers/hub.py:178\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39melif\u001b[39;00m path_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     model_path \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_find_model_path_from_repo(\n\u001b[1;32m    172\u001b[0m         model_path_or_repo_id,\n\u001b[1;32m    173\u001b[0m         model_file,\n\u001b[1;32m    174\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    175\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[0;32m--> 178\u001b[0m \u001b[39mreturn\u001b[39;00m LLM(\n\u001b[1;32m    179\u001b[0m     model_path\u001b[39m=\u001b[39;49mmodel_path,\n\u001b[1;32m    180\u001b[0m     model_type\u001b[39m=\u001b[39;49mmodel_type,\n\u001b[1;32m    181\u001b[0m     config\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    182\u001b[0m     lib\u001b[39m=\u001b[39;49mlib,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/ctransformers/llm.py:234\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lib\u001b[39m.\u001b[39mctransformers_llm_create(\n\u001b[1;32m    229\u001b[0m     model_path\u001b[39m.\u001b[39mencode(),\n\u001b[1;32m    230\u001b[0m     model_type\u001b[39m.\u001b[39mencode(),\n\u001b[1;32m    231\u001b[0m     config\u001b[39m.\u001b[39mto_struct(),\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to create LLM \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to create LLM 'llama' from '/home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin'."
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"/home/msaad/workspace/honors-thesis/fine-tuning/models/ggml/brockportgpt.ggmlv3.q4_0.bin\", model_type=\"llama\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
