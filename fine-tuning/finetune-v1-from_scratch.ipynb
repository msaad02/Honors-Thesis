{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning LLaMA 2 without guide\n",
    "\n",
    "I seem to keep running in to issues when following the guides and tutorials for finetuning LLaMA. It seems most people use other repos and tools to fine tune as opposed to writing their own scripts, and understandably so, but I would like to understand the process better and be able to do it myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import huggingface_hub\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 7098\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"msaad02/formatted-ss-cleaned-brockport-qa\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in base model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    ),\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# base_model.config.use_cache = False\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "# base_model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1, # increase dropout/test. Try 0.5?\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# See docs for explanations: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,                  # Directory for predictions and checkpoints\n",
    "    per_device_train_batch_size=16,         # Batch size per device - default 8. Need to find right setting for my hardware\n",
    "    gradient_accumulation_steps=2,          # Number of updates steps to accumulate the gradients for... Need to learn more. Default 1\n",
    "    learning_rate=2e-4,                     # Learning rate, default = 5e-5\n",
    "    logging_steps=10,                       # How often to log or print updates. User preference\n",
    "    num_train_epochs=2                      # Num epochs. Default 3\n",
    "    # max_steps=500                         # OVERRIDES num_train_epochs if set. \n",
    ")\n",
    "\n",
    "max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following other example from huggingface trl documentation\n",
    "\n",
    "https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/sft_llama2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "        \"msaad02/formatted-ss-cleaned-brockport-qa\",\n",
    "        data_dir=args.subset,\n",
    "        split=args.split,\n",
    "        use_auth_token=True,\n",
    "        num_proc=args.num_workers if not args.streaming else None,\n",
    "        streaming=args.streaming,\n",
    "    )\n",
    "\n",
    "train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        formatting_func=prepare_sample_text,\n",
    "        infinite=True,\n",
    "        seq_length=args.seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
